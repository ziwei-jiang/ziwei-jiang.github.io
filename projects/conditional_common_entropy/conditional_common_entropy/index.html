<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Conditional Common Entropy for Instrumental Variable Testing and Partial Identification | Ziwei's Site</title><meta name=keywords content="Causality"><meta name=description content=" 
Overview
Instrumental variables (IVs) are widely used for estimating causal effects. There are two main challenges when using instrumental variables. First of all, using IV without additional assumptions such as linearity, the causal effect may still not be identifiable. Second, when selecting an IV, the validity of the selected IV is typically not testable since the causal graph is not identifiable from observational data. In this work, we propose a method for bounding the causal effect with instrumental variables under weak confounding. In addition, we present a novel criterion to falsify the IV with side information about the confounder."><meta name=author content><link rel=canonical href=https://ziwei-jiang.github.io/projects/conditional_common_entropy/conditional_common_entropy/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.175f8a2522675f2652eb58ad47be19952cb91de9e66edc4b95bb19fff5546ef5.css integrity="sha256-F1+KJSJnXyZS61itR74ZlSy5HenmbtxLlbsZ//VUbvU=" rel="preload stylesheet" as=style><link rel=icon href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=icon type=image/png sizes=16x16 href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=icon type=image/png sizes=32x32 href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=apple-touch-icon href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=mask-icon href=https://ziwei-jiang.github.io/img/abra_icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ziwei-jiang.github.io/projects/conditional_common_entropy/conditional_common_entropy/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-H4P7KF2N8Z"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H4P7KF2N8Z")}</script><meta property="og:url" content="https://ziwei-jiang.github.io/projects/conditional_common_entropy/conditional_common_entropy/"><meta property="og:site_name" content="Ziwei's Site"><meta property="og:title" content="Conditional Common Entropy for Instrumental Variable Testing and Partial Identification"><meta property="og:description" content=" Overview Instrumental variables (IVs) are widely used for estimating causal effects. There are two main challenges when using instrumental variables. First of all, using IV without additional assumptions such as linearity, the causal effect may still not be identifiable. Second, when selecting an IV, the validity of the selected IV is typically not testable since the causal graph is not identifiable from observational data. In this work, we propose a method for bounding the causal effect with instrumental variables under weak confounding. In addition, we present a novel criterion to falsify the IV with side information about the confounder."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-07-18T11:30:03+00:00"><meta property="article:modified_time" content="2024-07-18T11:30:03+00:00"><meta property="article:tag" content="Causality"><meta property="og:image" content="https://ziwei-jiang.github.io/projects/conditional_common_entropy/conditional_common_entropy/projects/conditional_common_entropy/imgs/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ziwei-jiang.github.io/projects/conditional_common_entropy/conditional_common_entropy/projects/conditional_common_entropy/imgs/cover.png"><meta name=twitter:title content="Conditional Common Entropy for Instrumental Variable Testing and Partial Identification"><meta name=twitter:description content=" 
Overview
Instrumental variables (IVs) are widely used for estimating causal effects. There are two main challenges when using instrumental variables. First of all, using IV without additional assumptions such as linearity, the causal effect may still not be identifiable. Second, when selecting an IV, the validity of the selected IV is typically not testable since the causal graph is not identifiable from observational data. In this work, we propose a method for bounding the causal effect with instrumental variables under weak confounding. In addition, we present a novel criterion to falsify the IV with side information about the confounder."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://ziwei-jiang.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Conditional Common Entropy for Instrumental Variable Testing and Partial Identification","item":"https://ziwei-jiang.github.io/projects/conditional_common_entropy/conditional_common_entropy/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Conditional Common Entropy for Instrumental Variable Testing and Partial Identification","name":"Conditional Common Entropy for Instrumental Variable Testing and Partial Identification","description":" Overview Instrumental variables (IVs) are widely used for estimating causal effects. There are two main challenges when using instrumental variables. First of all, using IV without additional assumptions such as linearity, the causal effect may still not be identifiable. Second, when selecting an IV, the validity of the selected IV is typically not testable since the causal graph is not identifiable from observational data. In this work, we propose a method for bounding the causal effect with instrumental variables under weak confounding. In addition, we present a novel criterion to falsify the IV with side information about the confounder.\n","keywords":["Causality"],"articleBody":" Overview Instrumental variables (IVs) are widely used for estimating causal effects. There are two main challenges when using instrumental variables. First of all, using IV without additional assumptions such as linearity, the causal effect may still not be identifiable. Second, when selecting an IV, the validity of the selected IV is typically not testable since the causal graph is not identifiable from observational data. In this work, we propose a method for bounding the causal effect with instrumental variables under weak confounding. In addition, we present a novel criterion to falsify the IV with side information about the confounder.\nCheckout Our Paper and Codes Conditional Common Entropy for Instrumental Variable Testing and Partial Identification [code] Published in International Conference on Machine Learning (ICML), 2024\nInstrumental Variable Instrumental variable can be used to estimate the causal effect with observational data or in the inperfect compliance randomized experiments. The variable $Z$ is said to be an instrumental variable if (1) $Z$ is not independent of treatment variable $X$ (relevence), (2) $Z$ affects outcome variable $Y$ only through $X$ (exclusion), and (3) $Z$ is independent of the unobserved confounder (exchangeability). The last two assumptions are in general untestable since there is no conditional independence condition between $Z$ and $Y$ in the observational data. Under different assumption, instrumental vairables can be used to estimate the causal effect or the bounds of causal effects. In this paper, we focus on the following two assumptions.\nMarginal stochastic exclusion $$E[Y_{z,x}] = E[Y_{z’,x}]\\;\\; \\forall z,z’,x$$\nMarginal exchangeability $$ Z \\perp\\!\\!\\!\\!\\perp Y_{z,x}\\;\\; \\forall z,x $$\nUnder the above assumptions, the causal effect can be bounded by a closed-form expression, known as the natural bounds. The gap between bounds depends on the incompliance rate, so in some cases, the bounds of causal effects might be uninformative. Another challenge is that the assumption for instrumental variable is untestable. There exist some method to falsify the invalid instruments, but in general, one cannot confirm an instrumental varibale. So it is an interesting question about how to apply extra knowledge about the data to further verify the instrumental variables.\nIn this work, we derive a method using the side information of the latent confounder to (1) reject invalid instruments, (2) get tighter bounds of causal effect, (3) set up a sensitivity parameter for the violation of instrumental variable.\nCommon Entropy The exact common information was proposed by Kumar et al. (2014) to measure the common part of two random variables. Unlike mutual information, the exact common information measures the entropy of the simplest variable that explains the dependency between variables X and Y . This has also been referred to as the common entropy.\nFormally, the common entropy is defined as\n$$\\begin{align*} \\text{CE}(X;Y) \u0026:= \\min_{q(x,y,w)} H(W)\\\\ \u0026 \\text{s.t. } I(X;Y|W) = 0; \\\\ \u0026 q(x,y,w) \\text{ compatible with the obs} \\end{align*} $$\n$q(x,y,w)$ compatible with observed distribution means $$ \\begin{align*} \u0026\\sum_w q(x,y,w) = p(x,y), \\forall x,y; \\\\ \u0026 0\\leq q(x,y,w) \\leq 1, \\forall x,y,w \\\\ \u0026\\sum_{x,y,w} q(x,y,w) =1 .\\nonumber \\end{align*} $$\nFor a pair of variables $X,Y$ with joint distribution $P(X,Y)$, the common entropy is the minimum entropy $H(W)$ with joint distribution $P(X,Y,W)$ such that the joint distribution marginalize to $P(X,Y)$ and $X \\perp\\!\\!\\!\\!\\perp Y|W $.\nConditional Common Entropy We extend the idea of common entropy to more general cases.\nConditional common entropy of two random variables $Z, Y$ given $X$ with the joint probability distribution $P(X, Y, Z)$ is defined as follows: $$ \\begin{align*} CCE(Z;Y|X) \u0026:= \\min_{q(x,y,z,w)} H(W)\\\\ \u0026 \\text{s.t. } I(Z;Y|X,W) = 0; \\\\ \u0026 q(x,y,z,w) \\text{ compatible with the obs} \\end{align*} $$\nThen we developed the graph-specific conditional common entropy that can be used as a proxy for an edge strength.\nGraph-Specific CCE Let $P(\\mathbf{V})$ be the joint distribution over variables $\\mathbf{V}$ and Markov relative $\\mathcal{G}$. Let $Z, Y\\in \\mathbf{V}$ satisfies $(Z\\not\\perp\\!\\!\\!\\!\\perp_d Y|\\mathbf{X})$ in $\\mathcal{G}$ for a subset of variables $\\mathbf{X}\\subset\\mathbf{V}$. Let $(v\\leftrightarrow v’)$ or $(v\\rightarrow v’)$ be an edge in $\\mathcal{G}$ such that $(Z \\perp\\!\\!\\!\\!\\perp_d Y|\\mathbf{X})$ upon its deletion. Define the \\textbf{graph-specific conditional common entropy} $CCE_{\\mathcal{G},(v\\rightarrow v’)}$ to be the minimum entropy of $W$ for some $P(\\mathbf{V}\\cup {W})$ that compatible with the graph $\\mathcal{G}’$, where $\\mathcal{G}’ $ is the graph that replace $(v\\rightarrow v’)$ with $(v\\rightarrow W\\rightarrow v’)$. Similarly define $CCE_{\\mathcal{G},(v\\leftrightarrow v’)}$ for $(v\\leftrightarrow v’)$.\nIn words, the graph-specific conditional common entropy requires the variable $W$ to satisfies additional independence constraints according to the graph. For the IV graph, it requires the variable $W$ independent of the instrument $Z$. Therefore the $CCE_\\mathcal{IV}$ is a lower bound for the entropy of latent confounders.\nSimilarly, if the confoudner between $X,Y$ is observed, we can also quantify the direct effect from $Z$ to $Y$.\nEstimating the common entropy is NP-hard, so we propose the IV LatentSearch algorithm to approximate the graph-specific CCE.\nFirst we relax the indepdent constraint to form the objective function $$ \\mathcal{L} = I(Z;Y|X,W) +\\beta_0 H(W) + \\beta_1 I(W;Z).$$\nThe algorithm is as shown below.\nBy running the IV LatentSearch algorithm for different value of $\\beta_0, \\beta_1$, we get the varible $W$ with different entropy and different conditional mutual information as shown in the figure below.\nWe proved that the IV LatentSearch finds the stationary point of the relaxed objective function. The approximated CCE can be obtained by setting a threshold for $I(Z;Y|X,W)$ and $I(W;Z)$ and find the minimum value of $H(W)$ while the mutual information below the threshold.\nPartial Identification Assuming we know the upper bound of confounder’s entropy $H(U)\\leq \\theta$, we have the following Theorem for estimate the bounds of causal effect.\nFor variables $Z, X, Y$ with $|X|=n, |Y|=m$, and $|Z|=l$ and the compatible joint distribution $P(Z, X, Y)$. Assuming $CCE_{IV}(Z;Y|X,U) = \\phi$. The causal effect of $x_t$ on $y_o$ is bounded by $\\text{LB} \\leq P(y_o|do(x_t)) \\leq \\text{UB}$, where $$ \\begin{align*} \u0026\\text{LB}/\\text{UB} = \\min/\\max{\\left(\\sum_{jl} b_{ojl}P(z_l)\\right)}\\\\ \u0026\\text{subject to}\\\\ \u0026P(z_k) b_{itk} = P(y_i, x_t, z_k); \\forall i,k ; \\sum_{ij}b_{ijk}=1; \\forall k\\\\ \u0026 \\sum_{i} b_{ijk} = P(x_j|z_k); \\forall j,k; 0\\leq b_{ijk}\\leq 1 ; \\forall i,j,k \\\\ \u0026 \\sum_{ijk} b_{ijk}P(z_k)\\log\\left(\\frac{b_{ijk}P(z_k) }{(\\sum_{j’k’} b_{ij’k’}P(z_{k’}))(\\sum_{i’} b_{i’jk}P(z_k))}\\right) \\leq \\theta+\\phi. \\end{align*} $$ If the marginal exchangeability holds, we can replace the last inequality constraint with the following two: $$ \\begin{align*} \u0026\\sum_{ij} b_{ijk} \\log\\left(\\frac{ b_{ijk}}{(\\sum_{j’} b_{ij’k})(\\sum_{i’} b_{i’jk})}\\right) \\leq \\theta ; \\forall k, \\\\ \u0026\\sum_{ijk} b_{ijk}P(z_k) \\log\\left(\\frac{\\sum_{j’} b_{ij’k}}{(\\sum_{j’k} b_{ij’k}P(z_k))}\\right) \\leq \\phi. \\\\ \\end{align*} $$\nThis theorem allow us to estimate bounds of causal effect under different cases as shown in the figure below.\nHere $\\phi$ serves as a sensitivity parameter for the violation of IV assumption. The following plot shows an example of the affect of sensitivity parameter to the causal effect.\nFalsify Invalid IV As shown in the previous example, the CCE can be used to quantify the edge in a DAG. Assuming we know the upper bounds of confounder’s entropy, we can use this to check if a variable could satisfies the IV assumptions with the observed dataset. In our experiments, we show that CCE can be used to reject invalid IV effectively. And we show experimentally that when the entropy of confounder is unknown, it can be used to select the variable that is more likely to be the IV. For more detail about the experiments, please check our paper “Conditional Common Entropy for Instrumental Variable Testing and Partial Identification”.\n","wordCount":"1196","inLanguage":"en","image":"https://ziwei-jiang.github.io/projects/conditional_common_entropy/conditional_common_entropy/projects/conditional_common_entropy/imgs/cover.png","datePublished":"2024-07-18T11:30:03Z","dateModified":"2024-07-18T11:30:03Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://ziwei-jiang.github.io/projects/conditional_common_entropy/conditional_common_entropy/"},"publisher":{"@type":"Organization","name":"Ziwei's Site","logo":{"@type":"ImageObject","url":"https://ziwei-jiang.github.io/img/abra_icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ziwei-jiang.github.io/ accesskey=h title="Ziwei Jiang 姜子维 (Alt + H)"><img src=https://ziwei-jiang.github.io/img/abra_icon.png alt aria-label=logo height=35>Ziwei Jiang 姜子维</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ziwei-jiang.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ziwei-jiang.github.io/notes/ title=Notes><span>Notes</span></a></li><li><a href=https://ziwei-jiang.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://ziwei-jiang.github.io/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://ziwei-jiang.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Conditional Common Entropy for Instrumental Variable Testing and Partial Identification</h1><div class=post-meta><span title='2024-07-18 11:30:03 +0000 +0000'>July 18, 2024</span>&nbsp;·&nbsp;&nbsp;·&nbsp;<a href=/tags/causality> Causality</a></div></header><figure class=entry-cover1><img loading=lazy src=https://ziwei-jiang.github.io/projects/conditional_common_entropy/imgs/cover.png alt><p></p></figure><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#overview aria-label=Overview>Overview</a></li><li><a href=#checkout-our-paper-and-codes aria-label="Checkout Our Paper and Codes">Checkout Our Paper and Codes</a></li><li><a href=#instrumental-variable aria-label="Instrumental Variable">Instrumental Variable</a></li><li><a href=#common-entropy aria-label="Common Entropy">Common Entropy</a></li><li><a href=#conditional-common-entropy aria-label="Conditional Common Entropy">Conditional Common Entropy</a></li><li><a href=#graph-specific-cce aria-label="Graph-Specific CCE">Graph-Specific CCE</a></li><li><a href=#partial-identification aria-label="Partial Identification">Partial Identification</a></li><li><a href=#falsify-invalid-iv aria-label="Falsify Invalid IV">Falsify Invalid IV</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p> </p><h1 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h1><p>Instrumental variables (IVs) are widely used for estimating causal effects. There are two main challenges when using instrumental variables. First of all, using IV without additional assumptions such as linearity, the causal effect may still not be identifiable. Second, when selecting an IV, the validity of the selected IV is typically not testable since the causal graph is not identifiable from observational data. In this work, we propose a method for bounding the causal effect with instrumental variables under weak confounding. In addition, we present a novel criterion to falsify the IV with side information about the confounder.</p><h1 id=checkout-our-paper-and-codes>Checkout Our Paper and Codes<a hidden class=anchor aria-hidden=true href=#checkout-our-paper-and-codes>#</a></h1><p><a href="https://openreview.net/pdf?id=Wnni3cu39x"><em>Conditional Common Entropy for Instrumental Variable Testing and Partial Identification</em></a>
[<a href=https://github.com/ziwei-jiang/Conditional-Common-Entropy>code</a>]
Published in International Conference on Machine Learning (ICML), 2024</p><h1 id=instrumental-variable>Instrumental Variable<a hidden class=anchor aria-hidden=true href=#instrumental-variable>#</a></h1><p>Instrumental variable can be used to estimate the causal effect with observational data or in the inperfect compliance randomized experiments. The variable $Z$ is said to be an instrumental variable if (1) $Z$ is not independent of treatment variable $X$ (relevence), (2) $Z$ affects outcome variable $Y$ only through $X$ (exclusion), and (3) $Z$ is independent of the unobserved confounder (exchangeability). The last two assumptions are in general untestable since there is no conditional independence condition between $Z$ and $Y$ in the observational data. Under different assumption, instrumental vairables can be used to estimate the causal effect or the bounds of causal effects. In this paper, we focus on the following two assumptions.</p><p><strong>Marginal stochastic exclusion</strong>
$$E[Y_{z,x}] = E[Y_{z&rsquo;,x}]\;\; \forall z,z&rsquo;,x$$</p><p><strong>Marginal exchangeability</strong>
$$ Z \perp\!\!\!\!\perp Y_{z,x}\;\; \forall z,x $$</p><p>Under the above assumptions, the causal effect can be bounded by a closed-form expression, known as the natural bounds. The gap between bounds depends on the incompliance rate, so in some cases, the bounds of causal effects might be uninformative. Another challenge is that the assumption for instrumental variable is untestable. There exist some method to falsify the invalid instruments, but in general, one cannot confirm an instrumental varibale. So it is an interesting question about how to apply extra knowledge about the data to further verify the instrumental variables.</p><p>In this work, we derive a method using the side information of the latent confounder to (1) reject invalid instruments, (2) get tighter bounds of causal effect, (3) set up a sensitivity parameter for the violation of instrumental variable.</p><h1 id=common-entropy>Common Entropy<a hidden class=anchor aria-hidden=true href=#common-entropy>#</a></h1><p>The exact common information was proposed by Kumar et al. (2014) to measure the common part of two random variables. Unlike mutual information, the exact common information measures the entropy
of the simplest variable that explains the dependency between variables X and Y . This has also been referred to as the common entropy.</p><p>Formally, the common entropy is defined as</p><p>$$\begin{align*}
\text{CE}(X;Y) &:= \min_{q(x,y,w)} H(W)\\
& \text{s.t. } I(X;Y|W) = 0; \\
& q(x,y,w) \text{ compatible with the obs}
\end{align*}
$$</p><p>$q(x,y,w)$ compatible with observed distribution means
$$
\begin{align*}
&\sum_w q(x,y,w) = p(x,y), \forall x,y; \\
& 0\leq q(x,y,w) \leq 1, \forall x,y,w \\
&\sum_{x,y,w} q(x,y,w) =1 .\nonumber
\end{align*}
$$</p><p>For a pair of variables $X,Y$ with joint distribution $P(X,Y)$, the common entropy is the minimum entropy $H(W)$ with joint distribution $P(X,Y,W)$ such that the joint distribution marginalize to $P(X,Y)$ and $X \perp\!\!\!\!\perp Y|W $.</p><figure class=align-center><img loading=lazy src=/projects/conditional_common_entropy/imgs/common_entropy.png#center width=70%></figure><h1 id=conditional-common-entropy>Conditional Common Entropy<a hidden class=anchor aria-hidden=true href=#conditional-common-entropy>#</a></h1><p>We extend the idea of common entropy to more general cases.</p><p>Conditional common entropy of two random variables $Z, Y$ given $X$ with the joint probability distribution $P(X, Y, Z)$ is defined as follows:
$$
\begin{align*}
CCE(Z;Y|X) &:= \min_{q(x,y,z,w)} H(W)\\
& \text{s.t. } I(Z;Y|X,W) = 0; \\
& q(x,y,z,w) \text{ compatible with the obs}
\end{align*}
$$</p><p>Then we developed the graph-specific conditional common entropy that can be used as a proxy for an edge strength.</p><h1 id=graph-specific-cce>Graph-Specific CCE<a hidden class=anchor aria-hidden=true href=#graph-specific-cce>#</a></h1><p>Let $P(\mathbf{V})$ be the joint distribution over variables $\mathbf{V}$ and Markov relative $\mathcal{G}$. Let $Z, Y\in \mathbf{V}$ satisfies $(Z\not\perp\!\!\!\!\perp_d Y|\mathbf{X})$ in $\mathcal{G}$ for a subset of variables $\mathbf{X}\subset\mathbf{V}$. Let $(v\leftrightarrow v&rsquo;)$ or $(v\rightarrow v&rsquo;)$ be an edge in $\mathcal{G}$ such that $(Z \perp\!\!\!\!\perp_d Y|\mathbf{X})$ upon its deletion. Define the \textbf{graph-specific conditional common entropy} $CCE_{\mathcal{G},(v\rightarrow v&rsquo;)}$ to be the minimum entropy of $W$ for some $P(\mathbf{V}\cup {W})$ that compatible with the graph $\mathcal{G}&rsquo;$, where $\mathcal{G}&rsquo; $ is the graph that replace $(v\rightarrow v&rsquo;)$ with $(v\rightarrow W\rightarrow v&rsquo;)$. Similarly define $CCE_{\mathcal{G},(v\leftrightarrow v&rsquo;)}$ for $(v\leftrightarrow v&rsquo;)$.</p><p>In words, the graph-specific conditional common entropy requires the variable $W$ to satisfies additional independence constraints according to the graph. For the IV graph, it requires the variable $W$ independent of the instrument $Z$. Therefore the $CCE_\mathcal{IV}$ is a lower bound for the entropy of latent confounders.</p><figure class=align-center><img loading=lazy src=/projects/conditional_common_entropy/imgs/iv_graph_cce.png#center width=80%></figure><p>Similarly, if the confoudner between $X,Y$ is observed, we can also quantify the direct effect from $Z$ to $Y$.</p><figure class=align-center><img loading=lazy src=/projects/conditional_common_entropy/imgs/direct_graph_cce.png#center width=80%></figure><p>Estimating the common entropy is NP-hard, so we propose the IV LatentSearch algorithm to approximate the graph-specific CCE.</p><p>First we relax the indepdent constraint to form the objective function
$$ \mathcal{L} = I(Z;Y|X,W) +\beta_0 H(W) + \beta_1 I(W;Z).$$</p><p>The algorithm is as shown below.</p><figure class=align-center><img loading=lazy src=/projects/conditional_common_entropy/imgs/algorithm.png#center width=80%></figure><p>By running the IV LatentSearch algorithm for different value of $\beta_0, \beta_1$, we get the varible $W$ with different entropy and different conditional mutual information as shown in the figure below.</p><figure class=align-center><img loading=lazy src=/projects/conditional_common_entropy/imgs/cce_grid.png#center width=80%></figure><p>We proved that the IV LatentSearch finds the stationary point of the relaxed objective function. The approximated CCE can be obtained by setting a threshold for $I(Z;Y|X,W)$ and $I(W;Z)$ and find the minimum value of $H(W)$ while the mutual information below the threshold.</p><h1 id=partial-identification>Partial Identification<a hidden class=anchor aria-hidden=true href=#partial-identification>#</a></h1><p>Assuming we know the upper bound of confounder&rsquo;s entropy $H(U)\leq \theta$, we have the following Theorem for estimate the bounds of causal effect.</p><p>For variables $Z, X, Y$ with $|X|=n, |Y|=m$, and $|Z|=l$ and the compatible joint distribution $P(Z, X, Y)$. Assuming $CCE_{IV}(Z;Y|X,U) = \phi$. The causal effect of $x_t$ on $y_o$ is bounded by $\text{LB} \leq P(y_o|do(x_t)) \leq \text{UB}$, where
$$
\begin{align*}
&\text{LB}/\text{UB} = \min/\max{\left(\sum_{jl} b_{ojl}P(z_l)\right)}\\
&\text{subject to}\\
&amp;P(z_k) b_{itk} = P(y_i, x_t, z_k); \forall i,k ; \sum_{ij}b_{ijk}=1; \forall k\\
& \sum_{i} b_{ijk} = P(x_j|z_k); \forall j,k; 0\leq b_{ijk}\leq 1 ; \forall i,j,k \\
& \sum_{ijk} b_{ijk}P(z_k)\log\left(\frac{b_{ijk}P(z_k) }{(\sum_{j&rsquo;k&rsquo;} b_{ij&rsquo;k&rsquo;}P(z_{k&rsquo;}))(\sum_{i&rsquo;} b_{i&rsquo;jk}P(z_k))}\right) \leq \theta+\phi.
\end{align*}
$$
If the marginal exchangeability holds, we can replace the last inequality constraint with the following two:
$$
\begin{align*}
&\sum_{ij} b_{ijk} \log\left(\frac{ b_{ijk}}{(\sum_{j&rsquo;} b_{ij&rsquo;k})(\sum_{i&rsquo;} b_{i&rsquo;jk})}\right) \leq \theta ; \forall k, \\
&\sum_{ijk} b_{ijk}P(z_k) \log\left(\frac{\sum_{j&rsquo;} b_{ij&rsquo;k}}{(\sum_{j&rsquo;k} b_{ij&rsquo;k}P(z_k))}\right) \leq \phi. \\
\end{align*}
$$</p><p>This theorem allow us to estimate bounds of causal effect under different cases as shown in the figure below.</p><figure class=align-center><img loading=lazy src=/projects/conditional_common_entropy/imgs/partial_id.png#center width=100%></figure><p>Here $\phi$ serves as a sensitivity parameter for the violation of IV assumption. The following plot shows an example of the affect of sensitivity parameter to the causal effect.</p><figure class=align-center><img loading=lazy src=/projects/conditional_common_entropy/imgs/drug_effect.png#center width=100%></figure><h1 id=falsify-invalid-iv>Falsify Invalid IV<a hidden class=anchor aria-hidden=true href=#falsify-invalid-iv>#</a></h1><p>As shown in the previous example, the CCE can be used to quantify the edge in a DAG. Assuming we know the upper bounds of confounder&rsquo;s entropy, we can use this to check if a variable could satisfies the IV assumptions with the observed dataset. In our experiments, we show that CCE can be used to reject invalid IV effectively. And we show experimentally that when the entropy of confounder is unknown, it can be used to select the variable that is more likely to be the IV. For more detail about the experiments, please check our paper <a href="https://openreview.net/pdf?id=Wnni3cu39x">&ldquo;Conditional Common Entropy for Instrumental Variable Testing and Partial Identification&rdquo;</a>.</p><p><figure class=align-center><img loading=lazy src=/projects/conditional_common_entropy/imgs/exp_reject.png#center width=80%></figure><figure class=align-center><img loading=lazy src=/projects/conditional_common_entropy/imgs/exp_select.png#center width=80%></figure></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://ziwei-jiang.github.io/tags/causality/>Causality</a></li></ul><nav class=paginav><a class=prev href=https://ziwei-jiang.github.io/notes/q-learning/><span class=title>« Prev</span><br><span>Q-Learning.md</span>
</a><a class=next href=https://ziwei-jiang.github.io/projects/causal_effect_under_weak_confounding/weak_confounding/><span class=title>Next »</span><br><span>Approximate Causal Effect Identification under Weak Confounding</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://ziwei-jiang.github.io/>Ziwei's Site</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>