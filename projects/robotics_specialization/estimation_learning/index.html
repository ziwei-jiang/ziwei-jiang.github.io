<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Robotics Specialization: Estimation and Learning | Ziwei's Site</title><meta name=keywords content="Robotics"><meta name=description content="Learning and estimation for robots "><meta name=author content><link rel=canonical href=https://ziwei-jiang.github.io/projects/robotics_specialization/estimation_learning/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.175f8a2522675f2652eb58ad47be19952cb91de9e66edc4b95bb19fff5546ef5.css integrity="sha256-F1+KJSJnXyZS61itR74ZlSy5HenmbtxLlbsZ//VUbvU=" rel="preload stylesheet" as=style><link rel=icon href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=icon type=image/png sizes=16x16 href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=icon type=image/png sizes=32x32 href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=apple-touch-icon href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=mask-icon href=https://ziwei-jiang.github.io/img/abra_icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ziwei-jiang.github.io/projects/robotics_specialization/estimation_learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-H4P7KF2N8Z"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H4P7KF2N8Z")}</script><meta property="og:url" content="https://ziwei-jiang.github.io/projects/robotics_specialization/estimation_learning/"><meta property="og:site_name" content="Ziwei's Site"><meta property="og:title" content="Robotics Specialization: Estimation and Learning"><meta property="og:description" content="Learning and estimation for robots "><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2018-07-13T11:30:03+00:00"><meta property="article:modified_time" content="2018-07-13T11:30:03+00:00"><meta property="article:tag" content="Robotics"><meta property="og:image" content="https://ziwei-jiang.github.io/projects/robotics_specialization/estimation_learning/projects/robotics_specialization/imgs/course5/estimation_learning.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ziwei-jiang.github.io/projects/robotics_specialization/estimation_learning/projects/robotics_specialization/imgs/course5/estimation_learning.png"><meta name=twitter:title content="Robotics Specialization: Estimation and Learning"><meta name=twitter:description content="Learning and estimation for robots "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://ziwei-jiang.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Robotics Specialization: Estimation and Learning","item":"https://ziwei-jiang.github.io/projects/robotics_specialization/estimation_learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Robotics Specialization: Estimation and Learning","name":"Robotics Specialization: Estimation and Learning","description":"Learning and estimation for robots ","keywords":["Robotics"],"articleBody":"Here’s the content from the Robotics Specialization: Estimation and Learning offered by UPenn on Coursera.\nThis is the fifth course in the robotics specialization. This course is about topics related to machine learning and estimation in robotics.\nGaussian Model Learning 1D Gaussian Distribution The Gaussian distribution is a widely used probability distribution defined by two parameters, the mean (μ) and standard deviation (σ), which represent its central value and spread. It is characterized by a bell-shaped probability density function (PDF) and has some nice mathematical properties such as the product of two Gaussian distributions is also Gaussian. Additionally, the Central Limit Theorem states that the sum or average of a large number of independent and identically distributed random variables converges to a Gaussian distribution, making it suitable for modeling noise in measurement or uncertainty.\n$$ P(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)} $$ 1D Gaussian Distribution Multivariate Gaussian Distribution The Multivariate Gaussian is an extension of the 1D Gaussian distribution, allowing us to model complex multidimensional data by specifying a vector of means (μ) and a covariance matrix (Σ) that describes both the central tendency and interrelationships between variables. Just as the one-dimensional Gaussian distribution is characterized by its mean and standard deviation, the multivariate version is defined by these parameters and exhibits a bell-shaped, elliptical probability density function.\n$$P(\\mathbf{x}) = \\frac{1}{(2\\pi)^\\frac{D}{2}|\\Sigma|^\\frac{1}{2}}\\exp{\\left(-\\frac{1}{2}\\mathbf{(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}\\right)}$$\nThe covariance matrix is a symmetric matrix that describes how pairs of random variables in the distribution are related to each other. It describes both the variances (spread) of individual variables along the diagonal and the covariances (relationships) between pairs of variables off the diagonal. A covariance of zero implies that the variables are uncorrelated. $$ \\Sigma = \\begin{bmatrix} \\sigma_{x_1}^2 \u0026\u0026 \\sigma_{x_1}\\sigma_{x_2} \\\\ \\sigma_{x_2}\\sigma_{x_1} \u0026\u0026 \\sigma_{x_2}^2 \\end{bmatrix}$$\n2D Gaussian Distribution Maximum Likelihood Estimation (MLE) To determine the most probable parameters for the Gaussian distribution, Maximum Likelihood Estimation (MLE) is often used. For the 1D Gaussian, we want to estimate the mean (μ) and standard deviation (σ). More specifically, we want to find the parameters that maximize the conditional probability over the observational data. $$ \\hat{\\mu}, \\hat{\\sigma} = \\argmin_{\\mu,\\sigma} P(\\{x_i\\}|\\mu, \\sigma)$$\nIf we assume each $x_i$ is independently sampled, it becomes:\n$$ \\hat{\\mu}, \\hat{\\sigma} = \\argmin_{\\mu,\\sigma} \\prod_{i=1}^n P(x_i|\\mu, \\sigma)$$\nNext, we can take the natural log of the objective function, since the natural log is a monotonically non-decreasing function.\n$$ \\hat{\\mu}, \\hat{\\sigma} = \\argmin_{\\mu,\\sigma} \\left(\\ln \\prod_{i=1}^n P(x_i|\\mu, \\sigma)\\right) = \\argmin_{\\mu,\\sigma} \\sum_{i=1}^n \\left(\\ln P(x_i|\\mu, \\sigma)\\right)$$\nThen we get the objective function in terms of the following summation.\n$$ \\hat{\\mu}, \\hat{\\sigma} = \\argmin_{\\mu,\\sigma} \\sum_{i=1}^n \\left( \\frac{(x_i-\\mu)^2}{2\\sigma^2} + \\ln\\sigma + \\text{const} \\right) $$\nTake the derivative of the objective function with respect to $\\mu$ and $\\sigma$, and set them to zero, we get\n$$ \\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N x_i ,\\qquad \\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{i=1}^N (x_i-\\hat{\\mu})^2. $$\nWith similar steps, we can get the MLE solution of multivariate Gaussian distribution.\n$$ \\hat{\\mathbf{\\mu}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{x_i}, \\qquad \\hat{\\mathbf{\\sigma}}^2 = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x_i}-\\hat{\\mathbf{\\mu}})(\\mathbf{x_i}-\\hat{\\mathbf{\\mu}})^T $$\nMixture of Gaussians The Gaussian Mixture Model (GMM) extends the Gaussian (normal) distribution to represent complex data distributions by combining multiple Gaussian components. Each component is defined by its own set of parameters, including mean and covariance, allowing GMMs to capture non-symmetric patterns and multimodal data structures.\n$$ P(\\mathbf{x}) = \\sum_{k=1}^K w_k g_k (\\mathbf{x}|\\mathbf{\\mu_k}, \\mathbf{\\Sigma_k}) $$\nThe $g_k$ is the Gaussian component with $\\mathbf{\\mu_k}$ and $ \\mathbf{\\Sigma_k}$. The $w_k$ is the mixing coefficient with $w_k\u003e0$ and $\\sum_{k=1}^K w_k = 1$.\nThe black curve illustrates an example of Gaussian Mixture Model. One primary weakness is that GMMs can become computationally demanding when dealing with a large number of components or clusters, as each component introduces additional parameters to estimate, potentially leading to high computational complexity.\nExpectation-Maximization (EM) Algorithm The Expectation-Maximization (EM) algorithm is a computational technique that can be used to estimate the parameters of a Gaussian Mixture Model (GMM). The EM algorithm iteratively updates estimation for the means, covariances, and mixing coefficients of the components. In the Expectation step (E-step), the algorithm computes the expected values of the missing data, given the current parameter estimates. The Maximization step (M-step), then optimizes these parameter estimates based on the expected data. EM iterates between these two steps until convergence. The flowchart below summarizes the updates for the E-step and M-step for estimating GMM parameters.\nIn the E-step, we fix the parameter of the Gaussian distributions and update the latent parameter $z_k^i = \\frac{g_k(x_i|\\mu_k,\\Sigma_k)}{\\sum_{k=1}^Kg_k(x_i|\\mu_k,\\Sigma_k)}$. Here the latent variable can be interpreted as the probability of $x_i$ is generated from $k$-th distribution. The $z_k^i$ is then fixed in the M-step while the parameters of Gaussian distributions are updated.\nColor Learning and Target Detection In this assignment, we learn to use GMM for target detection using GMM.\nThe process of colored ball detection involves two steps. Firstly, we collect color samples of the yellow ball from training images and label them.\nSubsequently, we use this training data to estimate the model parameters, specifically the mean and covariance. Then, during the detection stage, given the model parameters and a test image, we calculate the likelihood of each pixel. Applying an appropriate threshold allows us to obtain a segmentation map of the yellow balls.\nBayesian Estimation - Target Tracking Kalman Filter Model Kalman filtering is a powerful algorithm that can the parameters of interest for dynamic systems. For example, the kalman filter can be used for tracking position of an object given a series of noisy measurements.\nUsing Kalman filter for stimating the true state of an object(red arrows), given the multiple noisy measurements. In this section, we consider a linear dynamic system of an object moving in the 1D space. Let $x_t$ be the state vector of the object and $z_t$ be the noisy measurement of $x_t$.\n$$ x_t = \\begin{bmatrix} s \u0026 \\frac{ds}{dt}\\end{bmatrix} \\qquad z_t = Cx_t.$$\nThe dynamic of the object is described by the transition matrix $A$.\n$$ x_{t+1} = Ax_t + B u_t \\qquad A = \\begin{bmatrix} 1 \u0026 dt \\\\ 0 \u0026 1 \\end{bmatrix} $$\nwhere $u_t$ represents all the external inputs that not depends on $x_t$.\nThe state space of dynamic system described above. At each time $t$, we want to estimate the current state $x_t$ given the previous state $x_{t-1}$ and the current measurement $z_t$. Since we can only estimate the state from the estimation of previous state and noisy measurement, we need to capture the uncertainty in our estimation. Using Baysian modeling, we can characterize the uncertainty with conditional probabilities:\n$ p(x_{t+1}|x_t)$ represent the prediction of the current state given the previous state, $ p(z_t|x_t)$ represent the probablity of measurement given the state, and the state vector can be modeled with a Gaussian distribution $ p(x_t) = \\mathcal{N}(x_t, \\Sigma_t)$.\nCombining the probability model and the previous linear dynamic model, we can get the following.\n$$ p(x_{t+1}|x_t) = A p(x_t) + \\nu_m = A\\mathcal{N}(x_t, \\Sigma_t) + \\mathcal{N}(0, \\Sigma_m), $$ $$ p(z_t|x_t) = C p(x_t) + \\nu_o = C\\mathcal{N}(x_t, \\Sigma_t) + \\mathcal{N}(0, \\Sigma_o),$$\nwhere $\\nu_m$ and $\\nu_o$ are the noise of estimation modeled by zero mean Gaussian distributions.\nThen apply the linearity and summation of Gaussian distributions.\n$$ p(x_{t+1}|x_t) = \\mathcal{N}(Ax_t, A\\Sigma_tA^T) + \\mathcal{N}(0, \\Sigma_m) = \\mathcal{N}(Ax_t, A\\Sigma_tA^T + \\Sigma_m)$$ $$ p(z_t|x_t) = \\mathcal{N}(Cx_t, C\\Sigma_tC^T) + \\mathcal{N}(0, \\Sigma_o) = \\mathcal{N}(Cx_t, C\\Sigma_tC^T + \\Sigma_o)$$\nMAP Estimate of KF After building the probabilistic model for the states and observations, we want to find the posterior distribution of the object given the previous state and observations.\n$$ p(x_t|z_t, x_{t-1}) = \\frac{p(z_t|x_t, x_{t-1}) p(x_t|x_{t-1})}{p(z_t)}$$\nHere $p(z_t|x_t, x_{t-1}) = A\\mathcal{N}(x_t, \\Sigma_t) + \\mathcal{N}(0, \\Sigma_m)$ is the prior distribution of the object state at time $t$. And $p(z_t|x_t) = C\\mathcal{N}(x_t, \\Sigma_t) + \\mathcal{N}(0, \\Sigma_o)$ is the likelihood. We can use the Maximum A Posteriori (MAP) to get a better estimation for $x_t$.\n$$ \\hat{x}_t = \\argmax _{x_t} p(x_t|z_t, x _{t-1}) = \\argmax _{x_t} p(z_t|x_t, x _{t-1}) p(x_t|x _{t-1})$$\n$$ \\hat{x}_t = \\argmax _{x_t} \\mathcal{N}(Cx_t, C\\Sigma_tC^T + \\Sigma_o) \\mathcal{N}(Ax_t, A\\Sigma_tA^T + \\Sigma_m)$$\nTake logorithm on both sides and using the following substitution for simplificiation.\n$P = \\Sigma_t = A\\Sigma_{t-1}A^T + \\Sigma_m, \\quad R = C\\Sigma_tC^T +\\Sigma_o$\nThen we have $$ \\hat{x}_t = \\argmin _{x_t} (z_t -Cx_t)R^{-1}(z_t-Cx_t) +(x_t-Ax _{t-1})P^{-1}(x_t -Ax _{t-1}).$$\nTaking the derivative with respect to $x_t$, and apply the Matrix Inversion Lemma, we can get the posterior estimation\n$$ \\hat{x}_t = Ax _{t-1} + K(z_t -CAx _{t-1}) ,$$\nwhere $K = PC^T(R+CPC^T)^{-1}$ is known as the Kalman gain.\nFor the exercise in this section, we learned to implement a Kalman filter for ball tracking in 2D space.\nMapping Occupancy Grid Mapping It is critical for mobile robots to know the surrounding environments. For the mobile robot moving in the 2D space, an intuitive way to describe the environment is Occupancy Grid Mapping, which represent the environment with a grid map. In general, there are two binary random variable associated with each cell in the grid map: the occupancy $m_{x,y}:\\{free, occupied\\} \\rightarrow \\{0,1\\}$ and the measurement $z\\sim \\{0,1\\}$.\nThe measurement can be obtained by the range sensor, which detect the obstacles in the path of rays. To characterize the uncertainty in the measurement, we can model the measurement using the conditional probability.\nA range sensor $$\\begin{align*} \u0026p(z=1|m_{x,y}=1) \\quad \\text{True occupied measurement} \\\\ \u0026p(z=0|m_{x,y}=1) \\quad \\text{False free measurement}\\\\ \u0026p(z=1|m_{x,y}=0) \\quad \\text{False occupied measurement}\\\\ \u0026p(z=0|m_{x,y}=0) \\quad \\text{True free measurement} \\end{align*}$$\nGiven our prior belief of the map and the measurement model, we can find the posterior map using Bayes’ rule.\nTo simplify the map update process, we can represent the map using log-odd values. The Odd of an event is defined as the ratio between the probability of the event happens and probability of the event not happens. In this case, the Odd is defined as following.\n$$ \\begin{align*}Odd((\\text{grid is occupied given z}) \u0026= \\frac{p(m_{x,y}=1|z)}{p(m_{x,y}=0|z)} \\\\ \u0026= \\frac{p(z|m_{x,y}=1)p(m_{x,y}=1)}{p(z|m_{x,y}=0)p(m_{x,y}=0)} \\\\ \\log Odd \u0026= \\log\\frac{p(z|m_{x,y}=1)}{p(z|m_{x,y}=0)} + \\log \\frac{p(m_{x,y}=1)}{p(m_{x,y}=0)}\\end{align*}$$\nThe first term is the log-odd of the measurement, and the second term is the log-odd of the prior map. At each time $t$, we can use log-odd update to get posterior map of the occupancy grid map by adding the new measurement to the current map.\nIn addition, we need to discretize the continuous value from distance sensor to the grid map. Given the size of grid $r$, the index of each cell is given by $i=ceil((x-x_{min})/r)$.\nIn practice, given the global frame of the map $(X_1, X_2)$ and the body frame of the robot at $(x_1, x_2)$ with orientation $\\theta$ and distance measurement $d$. We can relate the sensor measurement to the robot map using the rotation matrix.\n$$ \\begin{bmatrix} x_{1,occ} \\\\ x_{2,occ} \\end{bmatrix} = \\begin{bmatrix} \\cos\\theta \u0026 \\sin\\theta \\\\ -\\sin\\theta \u0026 \\cos\\theta \\end{bmatrix}\\begin{bmatrix} d\\\\ 0 \\end{bmatrix} + \\begin{bmatrix} x_1 \\\\ x_2\\end{bmatrix} $$\nAfter finding occupied cell $(x_{1,occ}, x_{2,occ})$ from the distance sensor, we can use the Bresenham’s line algorithm to calculate the index of the free cells. Often we will have multple range measurements from a robot. In that case, we can use the procedure described above to find occupied and free cells for each ray and update the map.\nIn the assignment, we apply the method discuss above to compute the map given the range data from lidar.\n3D Mapping To build a 3D map of environment, we can use sensors such as 3D range sensor, stereo camera, or depth camera to capture the 3D coordinate of the distance measures. One challenge of extending the 2D mapping method to 3D is the representation of the map. The occupancy grid map is not suitable in 3D case because empty space will be significantly large in 3D space compares to the occupied space. So there will be information losses in the discretization steps. And it is also not a memory efficient way to represent the map for the same reason.\nThe list representation of measured points is more efficient to represent map in 3D space, and it does not involves the discretization challenge. However, the number of points increase quickly as the map getting large, and it will take a long time to access or check the existing points in the map.\nTo overcome this issue, we want to find some structured way such as trees to represent the data in a list, so that we can access the existing data ponints quickly while maintan the sufficiency of the memory usage.\nAn example of such method is called Octree. An octree is a tree data structure used for partitioning 3D space by recursively subdividing it into eight octants. Each node in an octree represents a cubic region of space and can have up to eight children, corresponding to the eight subdivided regions. Octrees help optimize operations such as collision detection, visibility determination, and rendering by reducing the complexity of spatial calculations, enabling faster and more efficient processing of 3D data.\nLocalization Odometry Modeling In the previous section, we assume the position of the robot relative to the map is known from the data. The position of robot can be obtained through various methods such as GPS, Cell towers, Wifi hotspots, which provide an absolute coordinate of the robot. However, these positioning systems have different levels of accuracy due to the noisy of measurement. In many cases, the accuracy of the positioning system may not satisfies the requirement of the application such as robot navigation or self-driving car. On the other hand, the local information of the robot usually can be obtained with more precision, therefore can be integrated to get more accurate localization.\nAn simple example of odometry is the wheel encoder of mobile robot. Wheel encoders measure the rotation of the robot’s wheels, allowing the calculation of the distance traveled based on the wheel’s size. By tracking the rotation of each wheel over time, the robot’s movement can be estimated. This information can then be combined with data from other sensors such as Gyroscope to improve the overall accuracy of the robot’s position estimate.\nMap Registration In the localization problem, we have two sets of information: the map of environment, and the measurement from the robot. We need to register the local measurement from the robot to the known map.\nThe goal is to find the pose of the robot that best fit to the measurements. The odometry information provide a set of belief states that we can search over. More specifically, we want to maximize the correlation between laser obstacles and map obstacles, as well as the free spaces in the laser measurements and map. Next section we introduce the particle filter for modeling the pose of the robot given measurements.\nParticle Filters The particle filter represents a distribution with a set of samples (particles) instead of explicit parametric form.\nFor the localization problem, each particle is a pair of values pose and weight, where weight represent the confidence of the pose. Similar to the Kalman filter, during the robot motion, we update the particle given the odometry information.\nDuring the initial stage, often most of the particles have very small weights and do not provide useful information for the pose of robot. We can use the resampling process the make the particles better represent the distribution of the robot pose. The number of effective particles $n = (\\sum w_i)^2/ \\sum w_i^2$ can be used as a criterion for resampling. In brief, the resampling process will make the prune out lower weighted particles.\nIterative Closest Point The particle filter is not suitable in 3D space. Since the pose has six degree of freedom, it requires exponentially more points to be sampled compares to the ground mobile robot. An alternative way to register the map is to match the observation points to the map directly. This can be done using the Iterative Closest Point (ICP) algorithm. In general, ICP algorithm can be used to register two point sets $X$ and $Y$. This involves solving two problems: finding the translation and rotation between two points, and determine the correspondences between points.\nWe can use the EM algorithm to solve these two problem iteratively. In the E-step, we find the closest points of all measured points corresponding to the map. And in the M-step, we fix the point correspondence and find the translation and rotation that better match the two point sets. The algorithm will improve the registration of the points in each iterations. And we can repeat this steps until it converges. Since the convergence only on the local minimum, we should assumes the two points sets are close enough at the beginning. The ICP algorithm is suitable for computing the motion increment of the robot.\n(Images and codes are from Robotics: Estimation and Learning.)\n","wordCount":"2746","inLanguage":"en","image":"https://ziwei-jiang.github.io/projects/robotics_specialization/estimation_learning/projects/robotics_specialization/imgs/course5/estimation_learning.png","datePublished":"2018-07-13T11:30:03Z","dateModified":"2018-07-13T11:30:03Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://ziwei-jiang.github.io/projects/robotics_specialization/estimation_learning/"},"publisher":{"@type":"Organization","name":"Ziwei's Site","logo":{"@type":"ImageObject","url":"https://ziwei-jiang.github.io/img/abra_icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ziwei-jiang.github.io/ accesskey=h title="Ziwei Jiang 姜子维 (Alt + H)"><img src=https://ziwei-jiang.github.io/img/abra_icon.png alt aria-label=logo height=35>Ziwei Jiang 姜子维</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ziwei-jiang.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ziwei-jiang.github.io/notes/ title=Notes><span>Notes</span></a></li><li><a href=https://ziwei-jiang.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://ziwei-jiang.github.io/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://ziwei-jiang.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Robotics Specialization: Estimation and Learning</h1><div class=post-description>Learning and estimation for robots</div><div class=post-meta><span title='2018-07-13 11:30:03 +0000 +0000'>July 13, 2018</span>&nbsp;·&nbsp;&nbsp;·&nbsp;<a href=/tags/robotics> Robotics</a></div></header><figure class=entry-cover1><img loading=lazy src=https://ziwei-jiang.github.io/projects/robotics_specialization/imgs/course5/estimation_learning.png alt><p></p></figure><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#gaussian-model-learning aria-label="Gaussian Model Learning">Gaussian Model Learning</a><ul><li><a href=#1d-gaussian-distribution aria-label="1D Gaussian Distribution">1D Gaussian Distribution</a></li><li><a href=#multivariate-gaussian-distribution aria-label="Multivariate Gaussian Distribution">Multivariate Gaussian Distribution</a></li><li><a href=#maximum-likelihood-estimation-mle aria-label="Maximum Likelihood Estimation (MLE)">Maximum Likelihood Estimation (MLE)</a></li><li><a href=#mixture-of-gaussians aria-label="Mixture of Gaussians">Mixture of Gaussians</a></li><li><a href=#expectation-maximization-em-algorithm aria-label="Expectation-Maximization (EM) Algorithm">Expectation-Maximization (EM) Algorithm</a></li><li><a href=#color-learning-and-target-detection aria-label="Color Learning and Target Detection">Color Learning and Target Detection</a></li></ul></li><li><a href=#bayesian-estimation---target-tracking aria-label="Bayesian Estimation - Target Tracking">Bayesian Estimation - Target Tracking</a><ul><li><a href=#kalman-filter-model aria-label="Kalman Filter Model">Kalman Filter Model</a></li><li><a href=#map-estimate-of-kf aria-label="MAP Estimate of KF">MAP Estimate of KF</a></li></ul></li><li><a href=#mapping aria-label=Mapping>Mapping</a><ul><li><a href=#occupancy-grid-mapping aria-label="Occupancy Grid Mapping">Occupancy Grid Mapping</a></li><li><a href=#3d-mapping aria-label="3D Mapping">3D Mapping</a></li></ul></li><li><a href=#localization aria-label=Localization>Localization</a><ul><li><a href=#odometry-modeling aria-label="Odometry Modeling">Odometry Modeling</a></li><li><a href=#map-registration aria-label="Map Registration">Map Registration</a></li><li><a href=#particle-filters aria-label="Particle Filters">Particle Filters</a></li><li><a href=#iterative-closest-point aria-label="Iterative Closest Point">Iterative Closest Point</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>Here&rsquo;s the content from the <em>Robotics Specialization: Estimation and Learning</em> offered by UPenn on Coursera.</p><p>This is the fifth course in the robotics specialization. This course is about topics related to machine learning and estimation in robotics.</p><h1 id=gaussian-model-learning>Gaussian Model Learning<a hidden class=anchor aria-hidden=true href=#gaussian-model-learning>#</a></h1><h2 id=1d-gaussian-distribution>1D Gaussian Distribution<a hidden class=anchor aria-hidden=true href=#1d-gaussian-distribution>#</a></h2><p>The Gaussian distribution is a widely used probability distribution defined by two parameters, the mean (μ) and standard deviation (σ), which represent its central value and spread. It is characterized by a bell-shaped probability density function (PDF) and has some nice mathematical properties such as the product of two Gaussian distributions is also Gaussian. Additionally, the <strong>Central Limit Theorem</strong> states that the sum or average of a large number of independent and identically distributed random variables converges to a Gaussian distribution, making it suitable for modeling noise in measurement or uncertainty.</p><p>$$ P(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp{\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)} $$<figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/1d_gaussian.png#center width=70%><figcaption>1D Gaussian Distribution</figcaption></figure></p><h2 id=multivariate-gaussian-distribution>Multivariate Gaussian Distribution<a hidden class=anchor aria-hidden=true href=#multivariate-gaussian-distribution>#</a></h2><p>The Multivariate Gaussian is an extension of the 1D Gaussian distribution, allowing us to model complex multidimensional data by specifying a vector of means (μ) and a covariance matrix (Σ) that describes both the central tendency and interrelationships between variables. Just as the one-dimensional Gaussian distribution is characterized by its mean and standard deviation, the multivariate version is defined by these parameters and exhibits a bell-shaped, elliptical probability density function.</p><p>$$P(\mathbf{x}) = \frac{1}{(2\pi)^\frac{D}{2}|\Sigma|^\frac{1}{2}}\exp{\left(-\frac{1}{2}\mathbf{(x-\mu)^T\Sigma^{-1}(x-\mu)}\right)}$$</p><p>The covariance matrix is a symmetric matrix that describes how pairs of random variables in the distribution are related to each other. It describes both the variances (spread) of individual variables along the diagonal and the covariances (relationships) between pairs of variables off the diagonal. A covariance of zero implies that the variables are uncorrelated.
$$ \Sigma = \begin{bmatrix} \sigma_{x_1}^2 && \sigma_{x_1}\sigma_{x_2} \\ \sigma_{x_2}\sigma_{x_1} && \sigma_{x_2}^2 \end{bmatrix}$$</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/2d_gaussian.png#center width=70%><figcaption>2D Gaussian Distribution</figcaption></figure><h2 id=maximum-likelihood-estimation-mle>Maximum Likelihood Estimation (MLE)<a hidden class=anchor aria-hidden=true href=#maximum-likelihood-estimation-mle>#</a></h2><p>To determine the most probable parameters for the Gaussian distribution, <strong>Maximum Likelihood Estimation (MLE)</strong> is often used. For the 1D Gaussian, we want to estimate the mean (μ) and standard deviation (σ). More specifically, we want to find the parameters that maximize the conditional probability over the observational data.
$$ \hat{\mu}, \hat{\sigma} = \argmin_{\mu,\sigma} P(\{x_i\}|\mu, \sigma)$$</p><p>If we assume each $x_i$ is independently sampled, it becomes:</p><p>$$ \hat{\mu}, \hat{\sigma} = \argmin_{\mu,\sigma} \prod_{i=1}^n P(x_i|\mu, \sigma)$$</p><p>Next, we can take the natural log of the objective function, since the natural log is a monotonically non-decreasing function.</p><p>$$ \hat{\mu}, \hat{\sigma} = \argmin_{\mu,\sigma} \left(\ln \prod_{i=1}^n P(x_i|\mu, \sigma)\right) = \argmin_{\mu,\sigma} \sum_{i=1}^n \left(\ln P(x_i|\mu, \sigma)\right)$$</p><p>Then we get the objective function in terms of the following summation.</p><p>$$ \hat{\mu}, \hat{\sigma} = \argmin_{\mu,\sigma} \sum_{i=1}^n \left( \frac{(x_i-\mu)^2}{2\sigma^2} + \ln\sigma + \text{const} \right) $$</p><p>Take the derivative of the objective function with respect to $\mu$ and $\sigma$, and set them to zero, we get</p><p>$$ \hat{\mu} = \frac{1}{N}\sum_{i=1}^N x_i ,\qquad \hat{\sigma}^2 = \frac{1}{N}\sum_{i=1}^N (x_i-\hat{\mu})^2. $$</p><p>With similar steps, we can get the MLE solution of multivariate Gaussian distribution.</p><p>$$ \hat{\mathbf{\mu}} = \frac{1}{N}\sum_{i=1}^N \mathbf{x_i}, \qquad \hat{\mathbf{\sigma}}^2 = \frac{1}{N}\sum_{i=1}^N (\mathbf{x_i}-\hat{\mathbf{\mu}})(\mathbf{x_i}-\hat{\mathbf{\mu}})^T $$</p><h2 id=mixture-of-gaussians>Mixture of Gaussians<a hidden class=anchor aria-hidden=true href=#mixture-of-gaussians>#</a></h2><p>The <strong>Gaussian Mixture Model (GMM)</strong> extends the Gaussian (normal) distribution to represent complex data distributions by combining multiple Gaussian components. Each component is defined by its own set of parameters, including mean and covariance, allowing GMMs to capture non-symmetric patterns and multimodal data structures.</p><p>$$ P(\mathbf{x}) = \sum_{k=1}^K w_k g_k (\mathbf{x}|\mathbf{\mu_k}, \mathbf{\Sigma_k}) $$</p><p>The $g_k$ is the Gaussian component with $\mathbf{\mu_k}$ and $ \mathbf{\Sigma_k}$. The $w_k$ is the mixing coefficient with $w_k>0$ and $\sum_{k=1}^K w_k = 1$.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/gmm.png#center width=60%><figcaption>The black curve illustrates an example of Gaussian Mixture Model.</figcaption></figure><p>One primary weakness is that GMMs can become computationally demanding when dealing with a large number of components or clusters, as each component introduces additional parameters to estimate, potentially leading to high computational complexity.</p><h2 id=expectation-maximization-em-algorithm>Expectation-Maximization (EM) Algorithm<a hidden class=anchor aria-hidden=true href=#expectation-maximization-em-algorithm>#</a></h2><p>The Expectation-Maximization (EM) algorithm is a computational technique that can be used to estimate the parameters of a Gaussian Mixture Model (GMM). The EM algorithm iteratively updates estimation for the means, covariances, and mixing coefficients of the components. In the Expectation step (E-step), the algorithm computes the expected values of the missing data, given the current parameter estimates. The Maximization step (M-step), then optimizes these parameter estimates based on the expected data. EM iterates between these two steps until convergence. The flowchart below summarizes the updates for the E-step and M-step for estimating GMM parameters.</p><table><thead><tr><th style=text-align:center><img alt="Picture 1" loading=lazy src=/projects/robotics_specialization/imgs/course5/e-step.png></th><th style=text-align:center><img alt="Picture 2" loading=lazy src=/projects/robotics_specialization/imgs/course5/m-step.png></th></tr></thead><tbody></tbody></table><p>In the E-step, we fix the parameter of the Gaussian distributions and update the latent parameter $z_k^i = \frac{g_k(x_i|\mu_k,\Sigma_k)}{\sum_{k=1}^Kg_k(x_i|\mu_k,\Sigma_k)}$. Here the latent variable can be interpreted as the probability of $x_i$ is generated from $k$-th distribution. The $z_k^i$ is then fixed in the M-step while the parameters of Gaussian distributions are updated.</p><h2 id=color-learning-and-target-detection>Color Learning and Target Detection<a hidden class=anchor aria-hidden=true href=#color-learning-and-target-detection>#</a></h2><p>In this assignment, we learn to use GMM for target detection using GMM.</p><p>The process of colored ball detection involves two steps. Firstly, we collect color samples of the yellow ball from training images and label them.</p><table><thead><tr><th style=text-align:center><img alt="Picture 1" loading=lazy src=/projects/robotics_specialization/imgs/course5/training1.png></th><th style=text-align:center><img alt="Picture 2" loading=lazy src=/projects/robotics_specialization/imgs/course5/training2.png></th></tr></thead><tbody></tbody></table><p>Subsequently, we use this training data to estimate the model parameters, specifically the mean and covariance. Then, during the detection stage, given the model parameters and a test image, we calculate the likelihood of each pixel. Applying an appropriate threshold allows us to obtain a segmentation map of the yellow balls.</p><table><thead><tr><th style=text-align:center><img alt="Picture 1" loading=lazy src=/projects/robotics_specialization/imgs/course5/testing1.png></th><th style=text-align:center><img alt="Picture 2" loading=lazy src=/projects/robotics_specialization/imgs/course5/testing1_mask.png></th></tr></thead><tbody></tbody></table><table><thead><tr><th style=text-align:center><img alt="Picture 1" loading=lazy src=/projects/robotics_specialization/imgs/course5/testing2.png></th><th style=text-align:center><img alt="Picture 2" loading=lazy src=/projects/robotics_specialization/imgs/course5/testing2_mask.png></th></tr></thead><tbody></tbody></table><h1 id=bayesian-estimation---target-tracking>Bayesian Estimation - Target Tracking<a hidden class=anchor aria-hidden=true href=#bayesian-estimation---target-tracking>#</a></h1><h2 id=kalman-filter-model>Kalman Filter Model<a hidden class=anchor aria-hidden=true href=#kalman-filter-model>#</a></h2><p>Kalman filtering is a powerful algorithm that can the parameters of interest for dynamic systems. For example, the kalman filter can be used for tracking position of an object given a series of noisy measurements.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/kf_intuition.png#center width=90%><figcaption>Using Kalman filter for stimating the true state of an object(red arrows), given the multiple noisy measurements.</figcaption></figure><p>In this section, we consider a linear dynamic system of an object moving in the 1D space. Let $x_t$ be the state vector of the object and $z_t$ be the noisy measurement of $x_t$.</p><p>$$ x_t = \begin{bmatrix} s & \frac{ds}{dt}\end{bmatrix} \qquad z_t = Cx_t.$$</p><p>The dynamic of the object is described by the transition matrix $A$.</p><p>$$ x_{t+1} = Ax_t + B u_t \qquad A = \begin{bmatrix} 1 & dt \\ 0 & 1 \end{bmatrix} $$</p><p>where $u_t$ represents all the external inputs that not depends on $x_t$.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/state_space.png#center width=90%><figcaption>The state space of dynamic system described above. At each time $t$, we want to estimate the current state $x_t$ given the previous state $x_{t-1}$ and the current measurement $z_t$.</figcaption></figure><p>Since we can only estimate the state from the estimation of previous state and noisy measurement, we need to capture the uncertainty in our estimation. Using Baysian modeling, we can characterize the uncertainty with conditional probabilities:</p><p>$ p(x_{t+1}|x_t)$ represent the prediction of the current state given the previous state, $ p(z_t|x_t)$ represent the probablity of measurement given the state, and the state vector can be modeled with a Gaussian distribution $ p(x_t) = \mathcal{N}(x_t, \Sigma_t)$.</p><p>Combining the probability model and the previous linear dynamic model, we can get the following.</p><p>$$ p(x_{t+1}|x_t) = A p(x_t) + \nu_m = A\mathcal{N}(x_t, \Sigma_t) + \mathcal{N}(0, \Sigma_m), $$
$$ p(z_t|x_t) = C p(x_t) + \nu_o = C\mathcal{N}(x_t, \Sigma_t) + \mathcal{N}(0, \Sigma_o),$$</p><p>where $\nu_m$ and $\nu_o$ are the noise of estimation modeled by zero mean Gaussian distributions.</p><p>Then apply the linearity and summation of Gaussian distributions.</p><p>$$ p(x_{t+1}|x_t) = \mathcal{N}(Ax_t, A\Sigma_tA^T) + \mathcal{N}(0, \Sigma_m) = \mathcal{N}(Ax_t, A\Sigma_tA^T + \Sigma_m)$$
$$ p(z_t|x_t) = \mathcal{N}(Cx_t, C\Sigma_tC^T) + \mathcal{N}(0, \Sigma_o) = \mathcal{N}(Cx_t, C\Sigma_tC^T + \Sigma_o)$$</p><h2 id=map-estimate-of-kf>MAP Estimate of KF<a hidden class=anchor aria-hidden=true href=#map-estimate-of-kf>#</a></h2><p>After building the probabilistic model for the states and observations, we want to find the posterior distribution of the object given the previous state and observations.</p><p>$$ p(x_t|z_t, x_{t-1}) = \frac{p(z_t|x_t, x_{t-1}) p(x_t|x_{t-1})}{p(z_t)}$$</p><p>Here $p(z_t|x_t, x_{t-1}) = A\mathcal{N}(x_t, \Sigma_t) + \mathcal{N}(0, \Sigma_m)$ is the prior distribution of the object state at time $t$. And $p(z_t|x_t) = C\mathcal{N}(x_t, \Sigma_t) + \mathcal{N}(0, \Sigma_o)$ is the likelihood. We can use the Maximum A Posteriori (MAP) to get a better estimation for $x_t$.</p><p>$$ \hat{x}_t = \argmax _{x_t} p(x_t|z_t, x _{t-1}) = \argmax _{x_t} p(z_t|x_t, x _{t-1}) p(x_t|x _{t-1})$$</p><p>$$ \hat{x}_t = \argmax _{x_t} \mathcal{N}(Cx_t, C\Sigma_tC^T + \Sigma_o) \mathcal{N}(Ax_t, A\Sigma_tA^T + \Sigma_m)$$</p><p>Take logorithm on both sides and using the following substitution for simplificiation.</p><p>$P = \Sigma_t = A\Sigma_{t-1}A^T + \Sigma_m, \quad R = C\Sigma_tC^T +\Sigma_o$</p><p>Then we have
$$ \hat{x}_t = \argmin _{x_t} (z_t -Cx_t)R^{-1}(z_t-Cx_t) +(x_t-Ax _{t-1})P^{-1}(x_t -Ax _{t-1}).$$</p><p>Taking the derivative with respect to $x_t$, and apply the Matrix Inversion Lemma, we can get the posterior estimation</p><p>$$ \hat{x}_t = Ax _{t-1} + K(z_t -CAx _{t-1}) ,$$</p><p>where $K = PC^T(R+CPC^T)^{-1}$ is known as the Kalman gain.</p><p>For the exercise in this section, we learned to implement a Kalman filter for ball tracking in 2D space.</p><table><thead><tr><th style=text-align:center><img alt="Picture 1" loading=lazy src=/projects/robotics_specialization/imgs/course5/ball_tracking.png></th><th style=text-align:center><img alt="Picture 2" loading=lazy src=/projects/robotics_specialization/imgs/course5/ball_tracking_error.png></th></tr></thead><tbody></tbody></table><h1 id=mapping>Mapping<a hidden class=anchor aria-hidden=true href=#mapping>#</a></h1><h2 id=occupancy-grid-mapping>Occupancy Grid Mapping<a hidden class=anchor aria-hidden=true href=#occupancy-grid-mapping>#</a></h2><p>It is critical for mobile robots to know the surrounding environments. For the mobile robot moving in the 2D space, an intuitive way to describe the environment is Occupancy Grid Mapping, which represent the environment with a grid map. In general, there are two binary random variable associated with each cell in the grid map: the occupancy $m_{x,y}:\{free, occupied\} \rightarrow \{0,1\}$ and the measurement $z\sim \{0,1\}$.</p><p>The measurement can be obtained by the range sensor, which detect the obstacles in the path of rays. To characterize the uncertainty in the measurement, we can model the measurement using the conditional probability.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/range_sensor.png#center width=60%><figcaption>A range sensor</figcaption></figure><p>$$\begin{align*} &amp;p(z=1|m_{x,y}=1) \quad \text{True occupied measurement} \\
&amp;p(z=0|m_{x,y}=1) \quad \text{False free measurement}\\
&amp;p(z=1|m_{x,y}=0) \quad \text{False occupied measurement}\\
&amp;p(z=0|m_{x,y}=0) \quad \text{True free measurement} \end{align*}$$</p><p>Given our prior belief of the map and the measurement model, we can find the posterior map using Bayes&rsquo; rule.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/ogm.png#center width=80%></figure><p>To simplify the map update process, we can represent the map using log-odd values. The Odd of an event is defined as the ratio between the probability of the event happens and probability of the event not happens. In this case, the Odd is defined as following.</p><p>$$ \begin{align*}Odd((\text{grid is occupied given z}) &= \frac{p(m_{x,y}=1|z)}{p(m_{x,y}=0|z)} \\
&= \frac{p(z|m_{x,y}=1)p(m_{x,y}=1)}{p(z|m_{x,y}=0)p(m_{x,y}=0)} \\
\log Odd &= \log\frac{p(z|m_{x,y}=1)}{p(z|m_{x,y}=0)} + \log \frac{p(m_{x,y}=1)}{p(m_{x,y}=0)}\end{align*}$$</p><p>The first term is the log-odd of the measurement, and the second term is the log-odd of the prior map. At each time $t$, we can use log-odd update to get posterior map of the occupancy grid map by adding the new measurement to the current map.</p><p>In addition, we need to discretize the continuous value from distance sensor to the grid map. Given the size of grid $r$, the index of each cell is given by $i=ceil((x-x_{min})/r)$.</p><p>In practice, given the global frame of the map $(X_1, X_2)$ and the body frame of the robot at $(x_1, x_2)$ with orientation $\theta$ and distance measurement $d$. We can relate the sensor measurement to the robot map using the rotation matrix.</p><p>$$ \begin{bmatrix} x_{1,occ} \\ x_{2,occ} \end{bmatrix} = \begin{bmatrix} \cos\theta & \sin\theta \\ -\sin\theta & \cos\theta \end{bmatrix}\begin{bmatrix} d\\ 0 \end{bmatrix} + \begin{bmatrix} x_1 \\ x_2\end{bmatrix} $$</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/measurements_on_grid.png#center width=80%></figure><p>After finding occupied cell $(x_{1,occ}, x_{2,occ})$ from the distance sensor, we can use the <strong>Bresenham&rsquo;s line algorithm</strong> to calculate the index of the free cells. Often we will have multple range measurements from a robot. In that case, we can use the procedure described above to find occupied and free cells for each ray and update the map.</p><p>In the assignment, we apply the method discuss above to compute the map given the range data from lidar.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/mapping.gif#center width=60%></figure><h2 id=3d-mapping>3D Mapping<a hidden class=anchor aria-hidden=true href=#3d-mapping>#</a></h2><p>To build a 3D map of environment, we can use sensors such as 3D range sensor, stereo camera, or depth camera to capture the 3D coordinate of the distance measures. One challenge of extending the 2D mapping method to 3D is the representation of the map. The occupancy grid map is not suitable in 3D case because empty space will be significantly large in 3D space compares to the occupied space. So there will be information losses in the discretization steps. And it is also not a memory efficient way to represent the map for the same reason.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/3d_grids.png#center width=60%></figure><p>The list representation of measured points is more efficient to represent map in 3D space, and it does not involves the discretization challenge. However, the number of points increase quickly as the map getting large, and it will take a long time to access or check the existing points in the map.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/3d_list.png#center width=60%></figure><p>To overcome this issue, we want to find some structured way such as trees to represent the data in a list, so that we can access the existing data ponints quickly while maintan the sufficiency of the memory usage.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/3d_tree.png#center width=80%></figure><p>An example of such method is called Octree. An octree is a tree data structure used for partitioning 3D space by recursively subdividing it into eight octants. Each node in an octree represents a cubic region of space and can have up to eight children, corresponding to the eight subdivided regions. Octrees help optimize operations such as collision detection, visibility determination, and rendering by reducing the complexity of spatial calculations, enabling faster and more efficient processing of 3D data.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/octree.png#center width=80%></figure><h1 id=localization>Localization<a hidden class=anchor aria-hidden=true href=#localization>#</a></h1><h2 id=odometry-modeling>Odometry Modeling<a hidden class=anchor aria-hidden=true href=#odometry-modeling>#</a></h2><p>In the previous section, we assume the position of the robot relative to the map is known from the data. The position of robot can be obtained through various methods such as GPS, Cell towers, Wifi hotspots, which provide an absolute coordinate of the robot. However, these positioning systems have different levels of accuracy due to the noisy of measurement. In many cases, the accuracy of the positioning system may not satisfies the requirement of the application such as robot navigation or self-driving car. On the other hand, the local information of the robot usually can be obtained with more precision, therefore can be integrated to get more accurate localization.</p><p>An simple example of odometry is the wheel encoder of mobile robot. Wheel encoders measure the rotation of the robot&rsquo;s wheels, allowing the calculation of the distance traveled based on the wheel&rsquo;s size. By tracking the rotation of each wheel over time, the robot&rsquo;s movement can be estimated. This information can then be combined with data from other sensors such as Gyroscope to improve the overall accuracy of the robot&rsquo;s position estimate.</p><h2 id=map-registration>Map Registration<a hidden class=anchor aria-hidden=true href=#map-registration>#</a></h2><p>In the localization problem, we have two sets of information: the map of environment, and the measurement from the robot. We need to register the local measurement from the robot to the known map.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/map_measurements.png#center width=80%></figure><p>The goal is to find the pose of the robot that best fit to the measurements. The odometry information provide a set of belief states that we can search over. More specifically, we want to maximize the correlation between laser obstacles and map obstacles, as well as the free spaces in the laser measurements and map. Next section we introduce the particle filter for modeling the pose of the robot given measurements.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/map_registration.png#center width=45%></figure><h2 id=particle-filters>Particle Filters<a hidden class=anchor aria-hidden=true href=#particle-filters>#</a></h2><p>The particle filter represents a distribution with a set of samples (particles) instead of explicit parametric form.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/particle_filter.png#center width=60%></figure><p>For the localization problem, each particle is a pair of values pose and weight, where weight represent the confidence of the pose. Similar to the Kalman filter, during the robot motion, we update the particle given the odometry information.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/filter_update.png#center width=45%></figure><p>During the initial stage, often most of the particles have very small weights and do not provide useful information for the pose of robot. We can use the resampling process the make the particles better represent the distribution of the robot pose. The number of effective particles $n = (\sum w_i)^2/ \sum w_i^2$ can be used as a criterion for resampling. In brief, the resampling process will make the prune out lower weighted particles.</p><h2 id=iterative-closest-point>Iterative Closest Point<a hidden class=anchor aria-hidden=true href=#iterative-closest-point>#</a></h2><p>The particle filter is not suitable in 3D space. Since the pose has six degree of freedom, it requires exponentially more points to be sampled compares to the ground mobile robot. An alternative way to register the map is to match the observation points to the map directly. This can be done using the Iterative Closest Point (ICP) algorithm. In general, ICP algorithm can be used to register two point sets $X$ and $Y$. This involves solving two problems: finding the translation and rotation between two points, and determine the correspondences between points.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/icp.png#center width=50%></figure><p>We can use the EM algorithm to solve these two problem iteratively. In the E-step, we find the closest points of all measured points corresponding to the map. And in the M-step, we fix the point correspondence and find the translation and rotation that better match the two point sets. The algorithm will improve the registration of the points in each iterations. And we can repeat this steps until it converges. Since the convergence only on the local minimum, we should assumes the two points sets are close enough at the beginning. The ICP algorithm is suitable for computing the motion increment of the robot.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course5/icp_alg.png#center width=80%></figure><p>(Images and codes are from <a href=https://www.coursera.org/learn/robotics-learning/>Robotics: Estimation and Learning</a>.)</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://ziwei-jiang.github.io/tags/robotics/>Robotics</a></li></ul><nav class=paginav><a class=prev href=https://ziwei-jiang.github.io/projects/superquadric/superquadric/><span class=title>« Prev</span><br><span>Extended Superquadric</span>
</a><a class=next href=https://ziwei-jiang.github.io/projects/robotics_specialization/perception/><span class=title>Next »</span><br><span>Robotics Specialization: Perception</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://ziwei-jiang.github.io/>Ziwei's Site</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>