<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Robotics Specialization: Perception | Ziwei's Site</title><meta name=keywords content="Robotics"><meta name=description content="Camera model and computer vision for robots "><meta name=author content><link rel=canonical href=https://ziwei-jiang.github.io/projects/robotics_specialization/perception/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.175f8a2522675f2652eb58ad47be19952cb91de9e66edc4b95bb19fff5546ef5.css integrity="sha256-F1+KJSJnXyZS61itR74ZlSy5HenmbtxLlbsZ//VUbvU=" rel="preload stylesheet" as=style><link rel=icon href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=icon type=image/png sizes=16x16 href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=icon type=image/png sizes=32x32 href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=apple-touch-icon href=https://ziwei-jiang.github.io/img/abra_icon.png><link rel=mask-icon href=https://ziwei-jiang.github.io/img/abra_icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ziwei-jiang.github.io/projects/robotics_specialization/perception/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-H4P7KF2N8Z"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H4P7KF2N8Z")}</script><meta property="og:url" content="https://ziwei-jiang.github.io/projects/robotics_specialization/perception/"><meta property="og:site_name" content="Ziwei's Site"><meta property="og:title" content="Robotics Specialization: Perception"><meta property="og:description" content="Camera model and computer vision for robots "><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2017-01-15T11:30:03+00:00"><meta property="article:modified_time" content="2017-01-15T11:30:03+00:00"><meta property="article:tag" content="Robotics"><meta property="og:image" content="https://ziwei-jiang.github.io/projects/robotics_specialization/perception/projects/robotics_specialization/imgs/course4/perception.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ziwei-jiang.github.io/projects/robotics_specialization/perception/projects/robotics_specialization/imgs/course4/perception.png"><meta name=twitter:title content="Robotics Specialization: Perception"><meta name=twitter:description content="Camera model and computer vision for robots "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://ziwei-jiang.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Robotics Specialization: Perception","item":"https://ziwei-jiang.github.io/projects/robotics_specialization/perception/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Robotics Specialization: Perception","name":"Robotics Specialization: Perception","description":"Camera model and computer vision for robots ","keywords":["Robotics"],"articleBody":"Here’s the content from the Robotics Specialization: Perception offered by UPenn on Coursera.\nThis is the fourth course in the robotics specialization. Throughout this course, we learn the basics of computer vision that involve robotics.\nSingle-view Geometry Pinhole Camera Pinhole camera The pinhole camera is the most commonly used camera model for computer vision. It can be seen as a simple form of perspective projection as shown in the figure below, which refers to the way objects in a three-dimensional scene appear on a two-dimensional surface when viewed from a particular point or viewpoint. The image below shows a 2D example. The rays from an object converge to some points in the image plane. Here $Y$ is the height of the object in the scene and $y$ is the height of the object in the image plane, $Z$ is the distance of the object from the camera, and $f$ is the focal length.\nPerspective projection. The geometry of the pinhole camera model imposes the following constraint\n$$ \\frac{Y}{Z} = -\\frac{y}{f}. $$\nImage plane in front of lens. If we assume the image plane is in front of the lens, then we have the following expression to transform a measurement in the scene to coordinates in the image.\n$$ y = f\\frac{Y}{Z}. $$\nHomogeneous Coordinate An interesting problem in computer vision that involves perspective projection is the transformation of the three-dimensional world into a two-dimensional image. There are two fundamental concepts called vanishing point and vanishing line. The vanishing points are the points on that line where parallel lines from the scene converge, giving the illusion of depth and distance. The vanishing line is a horizontal line on the image plane that represents the viewer’s eye level. It serves as a reference for the convergence of parallel lines in the scene.\nThe figure below shows two vanishing points at the horizon line and vertical vanishing points at infinity. Vanishing points and vanishing line. Homogeneous coordinates are often used to represent both finite and infinite points and lines in the image. In addition, homogeneous coordinates make transformations, like rotation and translation more efficient to compute. In the homogeneous coordinates, each point in the image is a ray in projective space.\nA point in homogeneous coordinates. In the above figure, Each point $(x,y)$ on the plane is represented by a ray $(sx,sy,s)$. Since all points on the ray are equivalent, the homogeneous representation of the point is $(x, y, 1) \\approx (sx, sy, s)$.\n$$ \\begin{bmatrix} x\\\\ y \\end{bmatrix} \\xrightarrow[\\text{coordinates}]{\\text{homogeneous}} \\begin{bmatrix} x\\\\ y \\\\ 1\\end{bmatrix} $$\nSimilarly, the line is a plane of rays through the origin as shown in the figure below.\nA line in homogeneous coordinates. If $(a,b,c)$ is the surface normal of a plane, a line is all the rays $(x,y,z)$ satisfying the following equation. $$ax + by + cz = 0$$\nThe homogeneous representation of a line can be defined as the normal of that plane $(a,b,c)$. The line $(0,0, 1)$ represents the line at infinity.\nGiven two points $P_1$ and $P_2$, the line passing through both points is defined by $l=P_1 \\times P_2$. Similarly, the point at the intersection of two lines can be represented by the cross product of two lines $P = l_1\\times l_2$. This is called point-line duality.\nIn this homogeneous coordinate, we can represent the line at infinity as $(0, 0, 1)$. This encapsulates the concept of lines that are parallel in Euclidean space but intersect at infinity in the projective plane. Similarly, ideal points at infinity can be defined as (x, y, 0). The point/line at infinity is also known as the ideal point/line.\nIdeal point and Ideal line. Rotation and Translation Homogeneous coordinates are also valuable in expressing transformations between different coordinate frames. This scenario commonly occurs in many robotics applications. For instance, consider a point $P$ that can be represented in both the camera frame $^cP$ and the world frame $^wP$. The transition between these two coordinate frames can be effectively captured using a rotation matrix $^cR_w$ and a translation matrix $^cT_w$.\n$$ {}^cP = {}^cR_w {}^cP +{}^cT_w$$\nIn homogeneous coordinates, we can write the rotation and translation matrix as a single compact transformation matrix.\n$$ {}^cM_w = \\begin{bmatrix} \u0026 {}^cR_w \u0026 \u0026 {}^cT_w \\\\ 0\u0026 0\u0026 0 \u0026 1 \\end{bmatrix} $$\nIntrinsic Parameters Recall the relationship between image coordinates and world coordinates. $$y = \\frac{fY}{Z}$$ $$x = \\frac{fX}{Z}$$\nThis can be expressed in the matrix form.\n$$Z\\begin{bmatrix}x\\\\ y\\\\ 1\\end{bmatrix} = \\begin{bmatrix}f \u0026 0 \u0026 0 \\\\ 0 \u0026 f \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix}X\\\\ Y\\\\ Z \\end{bmatrix}$$\nLet $p_x$, $p_y$ denote the principle point, where the optical axis hits the image plane. And $\\alpha_x$, $\\alpha_y$ are the pixel scaling factor. We have the following relationships.\n$$\\begin{bmatrix}x\\\\ y\\\\ z\\end{bmatrix} = \\begin{bmatrix}\\alpha_x \u0026 s \u0026 p_x \\\\ 0 \u0026 \\alpha_y \u0026 p_y \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix}f \u0026 0 \u0026 0 \\\\ 0 \u0026 f \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix}X\\\\ Y\\\\ Z \\end{bmatrix}$$\n$$\\begin{bmatrix}x\\\\ y\\\\ z\\end{bmatrix} = \\begin{bmatrix}f_x \u0026 s \u0026 p_x \\\\ 0 \u0026 f_y \u0026 p_y \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix}X\\\\ Y\\\\ Z \\end{bmatrix} $$\nHere $s$ is a parameter that accounts for the image plane that is not normal to the optical axis. We can also represent the pixel coordinate $u_x$, $u_y$ in homogeneous coordinates.\n$$z\\begin{bmatrix}u_x\\\\ u_y\\\\ 1\\end{bmatrix} = \\begin{bmatrix}f_x \u0026 s \u0026 p_x \\\\ 0 \u0026 f_y \u0026 p_y \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} \\mathbf{I_{3\\times 3}} \u0026 \\mathbf{0_{3\\times 1}} \\end{bmatrix} \\begin{bmatrix}X\\\\ Y\\\\ Z \\\\ 1 \\end{bmatrix} = \\mathbf{K}\\begin{bmatrix} \\mathbf{I_{3\\times 3}} \u0026 \\mathbf{0_{3\\times 1}} \\end{bmatrix} \\begin{bmatrix}X\\\\ Y\\\\ Z \\\\ 1 \\end{bmatrix}$$\nThis matrix $\\mathbf{K}$ is called the intrinsic parameters of the camera model. This usually can be estimated through a procedure called camera calibration, which uses images of a checkerboard to compute the intrinsic parameters.\nExtrinsic Parameters To relate different camera positions with the extrinsic parameters, we can use the rotation and translation matrix to transform between different world coordinates.\n$$ \\begin{bmatrix} X’\\\\ Y’\\\\ Z’\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\mathbf{R_{3\\times 3}} \u0026 \\mathbf{t_{3\\times 1}} \\\\ \\mathbf{0} \u0026 1 \\end{bmatrix}\\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ 1 \\end{bmatrix}$$\nCombining this with the intrinsic parameters, we can get the following expression.\n$$ \\begin{bmatrix}u_x\\\\ u_y\\\\ 1\\end{bmatrix} = \\begin{bmatrix}f_x \u0026 s \u0026 p_x \\\\ 0 \u0026 f_y \u0026 p_y \\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} \\mathbf{I_{3\\times 3}} \u0026 \\mathbf{0_{3\\times 1}} \\end{bmatrix} \\begin{bmatrix} \\mathbf{R_{3\\times 3}} \u0026 \\mathbf{t_{3\\times 1}} \\\\ \\mathbf{0} \u0026 1 \\end{bmatrix} \\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ 1 \\end{bmatrix}$$\nThis can be written compactly relating the image coordinate and world coordinate through the camera parameters.\n$$ \\mathbf{x} = \\mathbf{K}\\begin{bmatrix} \\mathbf{R} , \\mathbf{t}\\end{bmatrix}\\mathbf{X} $$\nImage Projection Using Homographies One application of projective geometry is the image projection. Homography is a mathematical transformation that relates two images taken from different perspectives or under different camera conditions. It establishes a geometric relationship between points in one image and their corresponding points in another image when they are related by a planar surface.\nHomography between two images. In this assignment, we learn to project the logo from an image to a sequence of video frames. Given a sequence of frames in a video and the corners in each frame that the logo should project onto, we can estimate the homography from the corresponding points. $$\\lambda \\mathbf{x} _{logo} = \\mathbf{H} \\mathbf{x} _{video}$$\n$$ \\lambda \\begin{bmatrix} x\\\\ y\\\\ 1 \\end{bmatrix} _{logo} = \\begin{bmatrix} h _{11} \u0026 h _{12} \u0026 h _{13} \\\\ h _{21} \u0026 h _{22} \u0026 h _{23} \\\\h _{31} \u0026 h _{32} \u0026 1 \\end{bmatrix} \\begin{bmatrix} x\\\\ y\\\\ 1 \\end{bmatrix} _{video} $$\nSince the homography matrix has eight DoF and each corresponding point imposes two constraints, we need at least 4 points to estimate the homography.\nvideo with logo projection. Pose Estimation Similar to the 2D homography, we can estimate the camera pose by computing the homography from the image to the 3D world coordinate.\n$$ \\begin{bmatrix} u\\\\ v\\\\ w \\end{bmatrix} \\sim K\\begin{bmatrix} r_1 \u0026 r_2 \u0026 r_3 \u0026 T \\end{bmatrix} \\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ W \\end{bmatrix} $$\nIf we assume all the points in the world coordinate lie in the ground plane $Z=0$, the transformation becomes\n$$ \\begin{bmatrix} u\\\\ v\\\\ w \\end{bmatrix} \\sim K\\begin{bmatrix} r_1 \u0026 r_2 \u0026 T \\end{bmatrix} \\begin{bmatrix} X\\\\ Y\\\\ W \\end{bmatrix} $$.\nThen given the intrinsic matrix $K$ and the corresponding coordinates in each frame, we can compute the camera pose by solving for $\\mathbf{R}$ and $\\mathbf{T}$.\nIn this assignment, the goal is to project a 3D cube on an AprilTag in the video with the corner coordinates in the first frame given. Then we can apply the Kanade–Lucas–Tomasi tracker to get the corner coordinates in each consecutive frame and solve for the homography.\nAfter we solve the homograph, to ensure the rotation matrix in the first two columns is orthogonal, we need to use Singular Value Decomposition to get the estimation of $\\mathbf{R}$ and $\\mathbf{T}$. AR cube projection to an AprilTag. Multi-view Geometry Moving on to the task of estimating the camera pose from a collection of images, similar to human vision, when we possess images captured from the same scene, we can deduce the camera’s position relative to the scene. This process is facilitated by a fundamental concept known as Epipolar Geometry, which provides a structured framework for understanding the geometric relationships between multiple cameras observing a common scene. Epipolar geometry plays an important role in computer vision and 3D reconstruction.\nEpipolar Geometry In the typical configuration of epipolar geometry, two cameras observe a common point $X$ in the scene. As shown in the following figure, the point is projected onto the image planes of the cameras, denoted as $P_1$ and $P_2$. The line connecting the two camera centers is known as the baseline. The plane defined by the two camera centers and X is called the epipolar plane. The points where the baseline intersects the two image planes are known as the epipoles. The lines where the intersection of the epipolar plane and the two image planes are known as the epipolar lines. If we select the point $P_1$ as the reference point, we can establish a relationship between $P_2$ and $P_1$ using a combination of translation and rotation matrices. Furthermore, we can denote the world point in the first and second camera’s view as $X_1$ and $X_2$ respectively.\nAn illustration of the basic concepts in epipolar geometry. The constraint from the points $X_1$ and $X_2$ is shown in the following equation $$ X_2^T E X_1 = 0.$$\nThe matrix $E$ is called the Essential Matrix. The essential matrix encodes the rotation and translation between two points $X_1$ and $X_2$. Similarly, we have the constraint between the image point $x_1$ and $x_2$ as $$ x_2^TFx_1=0. $$\nThe matrix $F$ is called the Fundamental Matrix. It encodes the rotation and translation as well as the intrinsic parameters of the cameras. The fundamental matrix and essential matrix is related by $$ F = K^{-T}EK^{-1} .$$\nGiven two images for the same scene, we can use Scale-invariant feature transform (SIFT) to detect and describe local features of images. Point correspondence. The fundamental matrix has $8$ DoF, to estimate the fundamental matrix, we need at least 8 correspondence points. $$ \\begin{bmatrix} u_i^2 \u0026 v_i^2 \u0026 1 \\end{bmatrix}\\begin{bmatrix} f_{11} \u0026 f_{12} \u0026 f_{13} \\\\ f_{21} \u0026 f_{22} \u0026 f_{23} \\\\ f_{31} \u0026 f_{32} \u0026 f_{33} \\end{bmatrix}\\begin{bmatrix} u_i^1 \\\\ v_i^1 \\\\ 1 \\end{bmatrix} = 0$$\nAfter estimating the fundamental matrix, if we know the intrinsic parameter of the camera, we can estimate the essential matrix and recover the camera pose $R$ and $T$. The camera poses from the essential matrix do not uniquely define a world point $X$ in the homogeneous coordinate. To find the world point that corresponds to the point in the image, we need multiple sets of point measurements. Then we can reconstruct those rays, and try to find the intersection of the rays into the 3D space. This procedure is called Triangulation.\nStructure from Motion In this assignment, we implemented key elements of structure from motion (SfM) except for bundle adjustment. We are given three images of Levine Hall at the University of Pennsylvania, and correspondence points that are established by nearest neighbor search in SIFT local descriptors filtered by Random sample consensus (RANSAC) based on the fundamental matrix.\nThe following figures show the corresponding points in the images and the error in reprojection points.\nThe camera pose and reconstructed 3D points are shown in the following figure.\nCamera poses and reconstructed 3D points. (Images and codes are from Robotics: Perception.)\n","wordCount":"2088","inLanguage":"en","image":"https://ziwei-jiang.github.io/projects/robotics_specialization/perception/projects/robotics_specialization/imgs/course4/perception.png","datePublished":"2017-01-15T11:30:03Z","dateModified":"2017-01-15T11:30:03Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://ziwei-jiang.github.io/projects/robotics_specialization/perception/"},"publisher":{"@type":"Organization","name":"Ziwei's Site","logo":{"@type":"ImageObject","url":"https://ziwei-jiang.github.io/img/abra_icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ziwei-jiang.github.io/ accesskey=h title="Ziwei Jiang 姜子维 (Alt + H)"><img src=https://ziwei-jiang.github.io/img/abra_icon.png alt aria-label=logo height=35>Ziwei Jiang 姜子维</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ziwei-jiang.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ziwei-jiang.github.io/notes/ title=Notes><span>Notes</span></a></li><li><a href=https://ziwei-jiang.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://ziwei-jiang.github.io/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://ziwei-jiang.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Robotics Specialization: Perception</h1><div class=post-description>Camera model and computer vision for robots</div><div class=post-meta><span title='2017-01-15 11:30:03 +0000 +0000'>January 15, 2017</span>&nbsp;·&nbsp;&nbsp;·&nbsp;<a href=/tags/robotics> Robotics</a></div></header><figure class=entry-cover1><img loading=lazy src=https://ziwei-jiang.github.io/projects/robotics_specialization/imgs/course4/perception.png alt><p></p></figure><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#single-view-geometry aria-label="Single-view Geometry">Single-view Geometry</a><ul><li><a href=#pinhole-camera aria-label="Pinhole Camera">Pinhole Camera</a></li><li><a href=#homogeneous-coordinate aria-label="Homogeneous Coordinate">Homogeneous Coordinate</a></li><li><a href=#rotation-and-translation aria-label="Rotation and Translation">Rotation and Translation</a></li><li><a href=#intrinsic-parameters aria-label="Intrinsic Parameters">Intrinsic Parameters</a></li><li><a href=#extrinsic-parameters aria-label="Extrinsic Parameters">Extrinsic Parameters</a></li><li><a href=#image-projection-using-homographies aria-label="Image Projection Using Homographies">Image Projection Using Homographies</a></li></ul></li><li><a href=#pose-estimation aria-label="Pose Estimation">Pose Estimation</a></li><li><a href=#multi-view-geometry aria-label="Multi-view Geometry">Multi-view Geometry</a><ul><li><a href=#epipolar-geometry aria-label="Epipolar Geometry">Epipolar Geometry</a></li><li><a href=#structure-from-motion aria-label="Structure from Motion">Structure from Motion</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>Here&rsquo;s the content from the <em>Robotics Specialization: Perception</em> offered by UPenn on Coursera.</p><p>This is the fourth course in the robotics specialization. Throughout this course, we learn the basics of computer vision that involve robotics.</p><h1 id=single-view-geometry>Single-view Geometry<a hidden class=anchor aria-hidden=true href=#single-view-geometry>#</a></h1><h2 id=pinhole-camera>Pinhole Camera<a hidden class=anchor aria-hidden=true href=#pinhole-camera>#</a></h2><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/pinhole_camera.svg#center width=90%><figcaption>Pinhole camera</figcaption></figure><p>The <strong>pinhole camera</strong> is the most commonly used camera model for computer vision. It can be seen as a simple form of <strong>perspective projection</strong> as shown in the figure below, which refers to the way objects in a three-dimensional scene appear on a two-dimensional surface when viewed from a particular point or viewpoint.
The image below shows a 2D example. The rays from an object converge to some points in the image plane. Here $Y$ is the height of the object in the scene and $y$ is the height of the object in the image plane, $Z$ is the distance of the object from the camera, and $f$ is the focal length.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/perspective_projection.png#center width=70%><figcaption>Perspective projection.</figcaption></figure><p>The geometry of the pinhole camera model imposes the following constraint</p><p>$$ \frac{Y}{Z} = -\frac{y}{f}. $$</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/perspective_projection2.png#center width=66%><figcaption>Image plane in front of lens.</figcaption></figure><p>If we assume the image plane is in front of the lens, then we have the following expression to transform a measurement in the scene to coordinates in the image.</p><p>$$ y = f\frac{Y}{Z}. $$</p><h2 id=homogeneous-coordinate>Homogeneous Coordinate<a hidden class=anchor aria-hidden=true href=#homogeneous-coordinate>#</a></h2><p>An interesting problem in computer vision that involves perspective projection is the transformation of the three-dimensional world into a two-dimensional image. There are two fundamental concepts called vanishing point and vanishing line. The <strong>vanishing points</strong> are the points on that line where parallel lines from the scene converge, giving the illusion of depth and distance. The <strong>vanishing line</strong> is a horizontal line on the image plane that represents the viewer&rsquo;s eye level. It serves as a reference for the convergence of parallel lines in the scene.</p><p>The figure below shows two vanishing points at the horizon line and vertical vanishing points at infinity.<figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/vanishing_points.png#center width=90%><figcaption>Vanishing points and vanishing line.</figcaption></figure></p><p><strong>Homogeneous coordinates</strong> are often used to represent both finite and infinite points and lines in the image. In addition, homogeneous coordinates make transformations, like rotation and translation more efficient to compute. In the homogeneous coordinates, each point in the image is a ray in projective space.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/homogeneous_points.png#center width=70%><figcaption>A point in homogeneous coordinates.</figcaption></figure><p>In the above figure, Each point $(x,y)$ on the plane is represented by a ray $(sx,sy,s)$. Since all points on the ray are equivalent, the <strong>homogeneous representation of the point</strong> is $(x, y, 1) \approx (sx, sy, s)$.</p><p>$$ \begin{bmatrix} x\\ y \end{bmatrix} \xrightarrow[\text{coordinates}]{\text{homogeneous}} \begin{bmatrix} x\\ y \\ 1\end{bmatrix} $$</p><p>Similarly, the line is a plane of rays through the origin as shown in the figure below.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/homogeneous_lines.png#center width=60%><figcaption>A line in homogeneous coordinates.</figcaption></figure><p>If $(a,b,c)$ is the surface normal of a plane, a line is all the rays $(x,y,z)$ satisfying the following equation.
$$ax + by + cz = 0$$</p><p>The <strong>homogeneous representation of a line</strong> can be defined as the normal of that plane $(a,b,c)$. The line $(0,0, 1)$ represents the line at infinity.</p><p>Given two points $P_1$ and $P_2$, the line passing through both points is defined by $l=P_1 \times P_2$. Similarly, the point at the intersection of two lines can be represented by the cross product of two lines $P = l_1\times l_2$. This is called <strong>point-line duality</strong>.</p><table><thead><tr><th style=text-align:center><img alt="Two points form a line" loading=lazy src=/projects/robotics_specialization/imgs/course4/pts2lines.png></th><th style=text-align:center><img alt="Two lines form a point" loading=lazy src=/projects/robotics_specialization/imgs/course4/lines2pts.png></th></tr></thead><tbody></tbody></table><p>In this homogeneous coordinate, we can represent the line at infinity as $(0, 0, 1)$. This encapsulates the concept of lines that are parallel in Euclidean space but intersect at infinity in the projective plane. Similarly, ideal points at infinity can be defined as (x, y, 0). The point/line at infinity is also known as the <strong>ideal point/line</strong>.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/ideal.png#center width=100%><figcaption>Ideal point and Ideal line.</figcaption></figure><h2 id=rotation-and-translation>Rotation and Translation<a hidden class=anchor aria-hidden=true href=#rotation-and-translation>#</a></h2><p>Homogeneous coordinates are also valuable in expressing transformations between different coordinate frames. This scenario commonly occurs in many robotics applications. For instance, consider a point $P$ that can be represented in both the camera frame $^cP$ and the world frame $^wP$. The transition between these two coordinate frames can be effectively captured using a rotation matrix $^cR_w$ and a translation matrix $^cT_w$.</p><p>$$ {}^cP = {}^cR_w {}^cP +{}^cT_w$$</p><p>In homogeneous coordinates, we can write the rotation and translation matrix as a single compact transformation matrix.</p><p>$$ {}^cM_w = \begin{bmatrix} & {}^cR_w & & {}^cT_w \\ 0& 0& 0 & 1 \end{bmatrix} $$</p><h2 id=intrinsic-parameters>Intrinsic Parameters<a hidden class=anchor aria-hidden=true href=#intrinsic-parameters>#</a></h2><p>Recall the relationship between image coordinates and world coordinates.
$$y = \frac{fY}{Z}$$
$$x = \frac{fX}{Z}$$</p><p>This can be expressed in the matrix form.</p><p>$$Z\begin{bmatrix}x\\ y\\ 1\end{bmatrix} = \begin{bmatrix}f & 0 & 0 \\ 0 & f & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix}X\\ Y\\ Z \end{bmatrix}$$</p><p>Let $p_x$, $p_y$ denote the principle point, where the optical axis hits the image plane. And $\alpha_x$, $\alpha_y$ are the pixel scaling factor. We have the following relationships.</p><p>$$\begin{bmatrix}x\\ y\\ z\end{bmatrix} = \begin{bmatrix}\alpha_x & s & p_x \\ 0 & \alpha_y & p_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix}f & 0 & 0 \\ 0 & f & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix}X\\ Y\\ Z \end{bmatrix}$$</p><p>$$\begin{bmatrix}x\\ y\\ z\end{bmatrix} = \begin{bmatrix}f_x & s & p_x \\ 0 & f_y & p_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix}X\\ Y\\ Z \end{bmatrix} $$</p><p>Here $s$ is a parameter that accounts for the image plane that is not normal to the optical axis. We can also represent the pixel coordinate $u_x$, $u_y$ in homogeneous coordinates.</p><p>$$z\begin{bmatrix}u_x\\ u_y\\ 1\end{bmatrix} = \begin{bmatrix}f_x & s & p_x \\ 0 & f_y & p_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} \mathbf{I_{3\times 3}} & \mathbf{0_{3\times 1}} \end{bmatrix} \begin{bmatrix}X\\ Y\\ Z \\ 1 \end{bmatrix} = \mathbf{K}\begin{bmatrix} \mathbf{I_{3\times 3}} & \mathbf{0_{3\times 1}} \end{bmatrix} \begin{bmatrix}X\\ Y\\ Z \\ 1 \end{bmatrix}$$</p><p>This matrix $\mathbf{K}$ is called the <strong>intrinsic parameters</strong> of the camera model. This usually can be estimated through a procedure called <strong>camera calibration</strong>, which uses images of a checkerboard to compute the intrinsic parameters.</p><h2 id=extrinsic-parameters>Extrinsic Parameters<a hidden class=anchor aria-hidden=true href=#extrinsic-parameters>#</a></h2><p>To relate different camera positions with the extrinsic parameters, we can use the rotation and translation matrix to transform between different world coordinates.</p><p>$$ \begin{bmatrix} X&rsquo;\\ Y&rsquo;\\ Z&rsquo;\\ 1 \end{bmatrix} = \begin{bmatrix} \mathbf{R_{3\times 3}} & \mathbf{t_{3\times 1}} \\ \mathbf{0} & 1 \end{bmatrix}\begin{bmatrix} X\\ Y\\ Z\\ 1 \end{bmatrix}$$</p><p>Combining this with the intrinsic parameters, we can get the following expression.</p><p>$$ \begin{bmatrix}u_x\\ u_y\\ 1\end{bmatrix} = \begin{bmatrix}f_x & s & p_x \\ 0 & f_y & p_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} \mathbf{I_{3\times 3}} & \mathbf{0_{3\times 1}} \end{bmatrix} \begin{bmatrix} \mathbf{R_{3\times 3}} & \mathbf{t_{3\times 1}} \\ \mathbf{0} & 1 \end{bmatrix} \begin{bmatrix} X\\ Y\\ Z\\ 1 \end{bmatrix}$$</p><p>This can be written compactly relating the image coordinate and world coordinate through the camera parameters.</p><p>$$ \mathbf{x} = \mathbf{K}\begin{bmatrix} \mathbf{R} , \mathbf{t}\end{bmatrix}\mathbf{X} $$</p><h2 id=image-projection-using-homographies>Image Projection Using Homographies<a hidden class=anchor aria-hidden=true href=#image-projection-using-homographies>#</a></h2><p>One application of projective geometry is the image projection. Homography is a mathematical transformation that relates two images taken from different perspectives or under different camera conditions. It establishes a geometric relationship between points in one image and their corresponding points in another image when they are related by a planar surface.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/homography.png#center width=100%><figcaption>Homography between two images.</figcaption></figure><p>In this assignment, we learn to project the logo from an image to a sequence of video frames. Given a sequence of frames in a video and the corners in each frame that the logo should project onto, we can estimate the homography from the corresponding points.
$$\lambda \mathbf{x} _{logo} = \mathbf{H} \mathbf{x} _{video}$$</p><p>$$ \lambda \begin{bmatrix} x\\ y\\ 1 \end{bmatrix} _{logo} = \begin{bmatrix} h _{11} & h _{12} & h _{13} \\ h _{21} & h _{22} & h _{23} \\h _{31} & h _{32} & 1 \end{bmatrix} \begin{bmatrix} x\\ y\\ 1 \end{bmatrix} _{video} $$</p><p>Since the homography matrix has eight DoF and each corresponding point imposes two constraints, we need at least 4 points to estimate the homography.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/homography_video.gif#center width=100%><figcaption>video with logo projection.</figcaption></figure><h1 id=pose-estimation>Pose Estimation<a hidden class=anchor aria-hidden=true href=#pose-estimation>#</a></h1><p>Similar to the 2D homography, we can estimate the camera pose by computing the homography from the image to the 3D world coordinate.</p><p>$$ \begin{bmatrix} u\\ v\\ w \end{bmatrix} \sim K\begin{bmatrix} r_1 & r_2 & r_3 & T \end{bmatrix} \begin{bmatrix} X\\ Y\\ Z\\ W \end{bmatrix} $$</p><p>If we assume all the points in the world coordinate lie in the ground plane $Z=0$, the transformation becomes</p><p>$$ \begin{bmatrix} u\\ v\\ w \end{bmatrix} \sim K\begin{bmatrix} r_1 & r_2 & T \end{bmatrix} \begin{bmatrix} X\\ Y\\ W \end{bmatrix} $$.</p><p>Then given the intrinsic matrix $K$ and the corresponding coordinates in each frame, we can compute the camera pose by solving for $\mathbf{R}$ and $\mathbf{T}$.</p><p>In this assignment, the goal is to project a 3D cube on an AprilTag in the video with the corner coordinates in the first frame given. Then we can apply the Kanade–Lucas–Tomasi tracker to get the corner coordinates in each consecutive frame and solve for the homography.</p><p>After we solve the homograph, to ensure the rotation matrix in the first two columns is orthogonal, we need to use Singular Value Decomposition to get the estimation of $\mathbf{R}$ and $\mathbf{T}$.<figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/ar.gif#center width=80%><figcaption>AR cube projection to an AprilTag.</figcaption></figure></p><h1 id=multi-view-geometry>Multi-view Geometry<a hidden class=anchor aria-hidden=true href=#multi-view-geometry>#</a></h1><p>Moving on to the task of estimating the camera pose from a collection of images, similar to human vision, when we possess images captured from the same scene, we can deduce the camera&rsquo;s position relative to the scene. This process is facilitated by a fundamental concept known as <strong>Epipolar Geometry</strong>, which provides a structured framework for understanding the geometric relationships between multiple cameras observing a common scene. Epipolar geometry plays an important role in computer vision and 3D reconstruction.</p><h2 id=epipolar-geometry>Epipolar Geometry<a hidden class=anchor aria-hidden=true href=#epipolar-geometry>#</a></h2><p>In the typical configuration of epipolar geometry, two cameras observe a common point $X$ in the scene. As shown in the following figure, the point is projected onto the image planes of the cameras, denoted as $P_1$ and $P_2$. The line connecting the two camera centers is known as the <strong>baseline</strong>. The plane defined by the two camera centers and X is called the <strong>epipolar plane</strong>. The points where the baseline intersects the two image planes are known as the <strong>epipoles</strong>. The lines where the intersection of the epipolar plane and the two image planes are known as the <strong>epipolar lines</strong>. If we select the point $P_1$ as the reference point, we can establish a relationship between $P_2$ and $P_1$ using a combination of translation and rotation matrices. Furthermore, we can denote the world point in the first and second camera&rsquo;s view as $X_1$ and $X_2$ respectively.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/epipolar.png#center width=80%><figcaption>An illustration of the basic concepts in epipolar geometry.</figcaption></figure><p>The constraint from the points $X_1$ and $X_2$ is shown in the following equation
$$ X_2^T E X_1 = 0.$$</p><p>The matrix $E$ is called the <strong>Essential Matrix</strong>. The essential matrix encodes the rotation and translation between two points $X_1$ and $X_2$. Similarly, we have the constraint between the image point $x_1$ and $x_2$ as
$$ x_2^TFx_1=0. $$</p><p>The matrix $F$ is called the <strong>Fundamental Matrix</strong>. It encodes the rotation and translation as well as the intrinsic parameters of the cameras. The fundamental matrix and essential matrix is related by
$$ F = K^{-T}EK^{-1} .$$</p><p>Given two images for the same scene, we can use Scale-invariant feature transform (SIFT) to detect and describe local features of images.<figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/correspondence.png#center width=80%><figcaption>Point correspondence.</figcaption></figure></p><p>The fundamental matrix has $8$ DoF, to estimate the fundamental matrix, we need at least 8 correspondence points.
$$ \begin{bmatrix} u_i^2 & v_i^2 & 1 \end{bmatrix}\begin{bmatrix} f_{11} & f_{12} & f_{13} \\ f_{21} & f_{22} & f_{23} \\ f_{31} & f_{32} & f_{33} \end{bmatrix}\begin{bmatrix} u_i^1 \\ v_i^1 \\ 1 \end{bmatrix} = 0$$</p><p>After estimating the fundamental matrix, if we know the intrinsic parameter of the camera, we can estimate the essential matrix and recover the camera pose $R$ and $T$. The camera poses from the essential matrix do not uniquely define a world point $X$ in the homogeneous coordinate. To find the world point that corresponds to the point in the image, we need multiple sets of point measurements. Then we can reconstruct those rays, and try to find the intersection of the rays into the 3D space. This procedure is called <strong>Triangulation</strong>.</p><h2 id=structure-from-motion>Structure from Motion<a hidden class=anchor aria-hidden=true href=#structure-from-motion>#</a></h2><p>In this assignment, we implemented key elements of structure from motion (SfM) except for bundle adjustment. We are given three images of Levine Hall at the University of Pennsylvania, and correspondence points that are established by nearest neighbor search in SIFT local descriptors filtered by Random sample consensus (RANSAC) based on the fundamental matrix.</p><p>The following figures show the corresponding points in the images and the error in reprojection points.</p><table><thead><tr><th style=text-align:center><img alt="Picture 1" loading=lazy src=/projects/robotics_specialization/imgs/course4/p1.png></th><th style=text-align:center><img alt="Picture 2" loading=lazy src=/projects/robotics_specialization/imgs/course4/p2.png></th><th style=text-align:center><img alt="Picture 3" loading=lazy src=/projects/robotics_specialization/imgs/course4/p3.png></th></tr></thead><tbody></tbody></table><p>The camera pose and reconstructed 3D points are shown in the following figure.</p><figure class=align-center><img loading=lazy src=/projects/robotics_specialization/imgs/course4/sfm.gif#center width=80%><figcaption>Camera poses and reconstructed 3D points.</figcaption></figure><p>(Images and codes are from <a href=https://www.coursera.org/learn/robotics-perception/>Robotics: Perception</a>.)</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://ziwei-jiang.github.io/tags/robotics/>Robotics</a></li></ul><nav class=paginav><a class=prev href=https://ziwei-jiang.github.io/projects/robotics_specialization/estimation_learning/><span class=title>« Prev</span><br><span>Robotics Specialization: Estimation and Learning</span>
</a><a class=next href=https://ziwei-jiang.github.io/projects/robotics_specialization/computational_motion_planning/><span class=title>Next »</span><br><span>Robotics Specialization: Computational Motion Planning</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://ziwei-jiang.github.io/>Ziwei's Site</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>